{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e13428c",
      "metadata": {},
      "source": [
        "# Multi-Hop QA Dataset Embedding & Graph Construction\n",
        "\n",
        "## 목적\n",
        "HotpotQA와 2WikiMultiHopQA 데이터셋에 대해 Step별(250, 500, 750, 1000) 임베딩 및 그래프를 구축\n",
        "\n",
        "## 출력 경로\n",
        "- `_hippo_rag_MHQA_CL/hotpotqa/step_{N}/`\n",
        "- `_hippo_rag_MHQA_CL/2wikimultihopqa/step_{N}/`\n",
        "\n",
        "## 입력 데이터\n",
        "- **Dataset**: `reproduce/dataset/{dataset}.json`, `reproduce/dataset/{dataset}_corpus.json`\n",
        "- **OpenIE**: `outputs/hipporag_hotpotqa/openie_results_ner_gpt-4o-mini.json` 등"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4e53295e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/minyeol/.conda/envs/hipporagenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2026-02-02 13:53:56,037\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 모듈 임포트 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. Imports & Configuration\n",
        "# ============================================================\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import gc\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Set, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# HippoRAG 경로 설정 (모듈이 src/ 안에 있음)\n",
        "import sys\n",
        "sys.path.append(\"/NAS/minyeol/hippoRAG/src\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,3\"\n",
        "\n",
        "from hipporag.HippoRAG import HippoRAG, compute_mdhash_id\n",
        "from hipporag.utils.config_utils import BaseConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "02d4b368",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 설정 완료\n",
            "  PROJECT_ROOT: /NAS/minyeol/hippoRAG\n",
            "  Steps: [250, 500, 750, 1000]\n",
            "  Datasets: ['hotpotqa', '2wikimultihopqa']\n",
            "\n",
            "경로 검증:\n",
            "  [hotpotqa]\n",
            "    samples: ✓ /NAS/minyeol/hippoRAG/reproduce/dataset/hotpotqa.json\n",
            "    corpus:  ✓ /NAS/minyeol/hippoRAG/reproduce/dataset/hotpotqa_corpus.json\n",
            "    openie:  ✓ /NAS/minyeol/hippoRAG/outputs/hotpotqa/openie_results_ner_gpt-4o-mini.json\n",
            "  [2wikimultihopqa]\n",
            "    samples: ✓ /NAS/minyeol/hippoRAG/reproduce/dataset/2wikimultihopqa.json\n",
            "    corpus:  ✓ /NAS/minyeol/hippoRAG/reproduce/dataset/2wikimultihopqa_corpus.json\n",
            "    openie:  ✓ /NAS/minyeol/hippoRAG/outputs/2wikimultihopqa/openie_results_ner_gpt-4o-mini.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. Configuration\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "데이터셋별 설정\n",
        "- 경로, Step 값, LLM/Embedding 모델 설정\n",
        "\n",
        "주의: 노트북이 _hippo_rag_MHQA_CL/ 폴더에 있으므로 절대경로 사용\n",
        "\"\"\"\n",
        "\n",
        "# 프로젝트 루트 경로 (절대경로)\n",
        "PROJECT_ROOT = \"/NAS/minyeol/hippoRAG\"\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    \"\"\"데이터셋 설정\"\"\"\n",
        "    name: str                    # 데이터셋 이름 (hotpotqa, 2wikimultihopqa)\n",
        "    samples_path: str            # 샘플 JSON 경로\n",
        "    corpus_path: str             # 코퍼스 JSON 경로\n",
        "    openie_path: str             # OpenIE 결과 JSON 경로\n",
        "    output_base_dir: str         # 출력 베이스 디렉토리\n",
        "    \n",
        "    @property\n",
        "    def output_dir(self) -> str:\n",
        "        return f\"{self.output_base_dir}/{self.name}\"\n",
        "\n",
        "\n",
        "# 데이터셋 설정 (절대경로 사용)\n",
        "DATASETS = {\n",
        "    \"hotpotqa\": DatasetConfig(\n",
        "        name=\"hotpotqa\",\n",
        "        samples_path=f\"{PROJECT_ROOT}/reproduce/dataset/hotpotqa.json\",\n",
        "        corpus_path=f\"{PROJECT_ROOT}/reproduce/dataset/hotpotqa_corpus.json\",\n",
        "        openie_path=f\"{PROJECT_ROOT}/outputs/hotpotqa/openie_results_ner_gpt-4o-mini.json\",\n",
        "        output_base_dir=f\"{PROJECT_ROOT}/_hippo_rag_MHQA_CL\",\n",
        "    ),\n",
        "    \"2wikimultihopqa\": DatasetConfig(\n",
        "        name=\"2wikimultihopqa\",\n",
        "        samples_path=f\"{PROJECT_ROOT}/reproduce/dataset/2wikimultihopqa.json\",\n",
        "        corpus_path=f\"{PROJECT_ROOT}/reproduce/dataset/2wikimultihopqa_corpus.json\",\n",
        "        openie_path=f\"{PROJECT_ROOT}/outputs/2wikimultihopqa/openie_results_ner_gpt-4o-mini.json\",\n",
        "        output_base_dir=f\"{PROJECT_ROOT}/_hippo_rag_MHQA_CL\",\n",
        "    ),\n",
        "}\n",
        "\n",
        "# 공통 설정\n",
        "STEPS = [250, 500, 750, 1000]\n",
        "LLM_NAME = \"gpt-4o-mini\"\n",
        "EMBEDDING_MODEL_NAME = \"nvidia/NV-Embed-v2\"\n",
        "\n",
        "print(\"✓ 설정 완료\")\n",
        "print(f\"  PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "print(f\"  Steps: {STEPS}\")\n",
        "print(f\"  Datasets: {list(DATASETS.keys())}\")\n",
        "\n",
        "# 경로 검증\n",
        "print(\"\\n경로 검증:\")\n",
        "for name, config in DATASETS.items():\n",
        "    samples_ok = \"✓\" if os.path.exists(config.samples_path) else \"❌\"\n",
        "    corpus_ok = \"✓\" if os.path.exists(config.corpus_path) else \"❌\"\n",
        "    openie_ok = \"✓\" if os.path.exists(config.openie_path) else \"❌\"\n",
        "    print(f\"  [{name}]\")\n",
        "    print(f\"    samples: {samples_ok} {config.samples_path}\")\n",
        "    print(f\"    corpus:  {corpus_ok} {config.corpus_path}\")\n",
        "    print(f\"    openie:  {openie_ok} {config.openie_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8edd423f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. Utility Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "데이터 로드, Gold 문서 추출 등 유틸리티 함수\n",
        "\"\"\"\n",
        "\n",
        "def load_json(path: str) -> Any:\n",
        "    \"\"\"JSON 파일 로드\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def save_json(data: Any, path: str) -> None:\n",
        "    \"\"\"JSON 파일 저장\"\"\"\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "def get_gold_docs(samples: List[Dict], dataset_name: str = None) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    샘플에서 정답 문서(supporting facts) 추출\n",
        "    \n",
        "    Args:\n",
        "        samples: 데이터셋 샘플 리스트\n",
        "        dataset_name: 데이터셋 이름 (hotpotqa는 특수 처리)\n",
        "    \n",
        "    Returns:\n",
        "        각 샘플별 정답 문서 리스트\n",
        "    \"\"\"\n",
        "    gold_docs = []\n",
        "    for sample in samples:\n",
        "        if 'supporting_facts' in sample:\n",
        "            gold_title = set([item[0] for item in sample['supporting_facts']])\n",
        "            gold_title_and_content_list = [item for item in sample['context'] if item[0] in gold_title]\n",
        "            if dataset_name and dataset_name.startswith('hotpotqa'):\n",
        "                gold_doc = [item[0] + '\\n' + ''.join(item[1]) for item in gold_title_and_content_list]\n",
        "            else:\n",
        "                gold_doc = [item[0] + '\\n' + ' '.join(item[1]) for item in gold_title_and_content_list]\n",
        "        elif 'contexts' in sample:\n",
        "            gold_doc = [item['title'] + '\\n' + item['text'] for item in sample['contexts'] if item['is_supporting']]\n",
        "        else:\n",
        "            assert 'paragraphs' in sample\n",
        "            gold_paragraphs = []\n",
        "            for item in sample['paragraphs']:\n",
        "                if 'is_supporting' in item and item['is_supporting'] is False:\n",
        "                    continue\n",
        "                gold_paragraphs.append(item)\n",
        "            gold_doc = [\n",
        "                item['title'] + '\\n' + (item['text'] if 'text' in item else item['paragraph_text']) \n",
        "                for item in gold_paragraphs\n",
        "            ]\n",
        "        gold_doc = list(set(gold_doc))\n",
        "        gold_docs.append(gold_doc)\n",
        "    return gold_docs\n",
        "\n",
        "\n",
        "def get_gold_answers(samples: List[Dict]) -> List[Set[str]]:\n",
        "    \"\"\"샘플에서 정답 추출\"\"\"\n",
        "    gold_answers = []\n",
        "    for sample in samples:\n",
        "        gold_ans = None\n",
        "        if 'answer' in sample or 'gold_ans' in sample:\n",
        "            gold_ans = sample.get('answer') or sample.get('gold_ans')\n",
        "        elif 'reference' in sample:\n",
        "            gold_ans = sample['reference']\n",
        "        elif 'obj' in sample:\n",
        "            gold_ans = set([sample['obj']] + [sample.get('possible_answers', '')] + \n",
        "                          [sample.get('o_wiki_title', '')] + [sample.get('o_aliases', '')])\n",
        "            gold_ans = list(gold_ans)\n",
        "        \n",
        "        assert gold_ans is not None\n",
        "        if isinstance(gold_ans, str):\n",
        "            gold_ans = [gold_ans]\n",
        "        gold_ans = set(gold_ans)\n",
        "        if 'answer_aliases' in sample:\n",
        "            gold_ans.update(sample['answer_aliases'])\n",
        "        gold_answers.append(gold_ans)\n",
        "    return gold_answers\n",
        "\n",
        "\n",
        "def collect_paragraphs_for_step(\n",
        "    samples: List[Dict], \n",
        "    corpus: List[Dict], \n",
        "    step: int,\n",
        "    dataset_name: str\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    특정 Step까지의 샘플에서 필요한 문서들 수집\n",
        "    \n",
        "    Args:\n",
        "        samples: 전체 샘플 리스트\n",
        "        corpus: 전체 코퍼스\n",
        "        step: 수집할 step 수 (처음 N개 샘플)\n",
        "        dataset_name: 데이터셋 이름\n",
        "    \n",
        "    Returns:\n",
        "        문서 리스트 (title + text 형식)\n",
        "    \"\"\"\n",
        "    step_samples = samples[:step]\n",
        "    needed_paragraphs = set()\n",
        "    \n",
        "    for sample in step_samples:\n",
        "        if 'paragraphs' in sample:\n",
        "            for para in sample['paragraphs']:\n",
        "                text = para.get('paragraph_text') or para.get('text', '')\n",
        "                if text:\n",
        "                    needed_paragraphs.add(text)\n",
        "        elif 'context' in sample:\n",
        "            for ctx in sample['context']:\n",
        "                if isinstance(ctx, list) and len(ctx) >= 2:\n",
        "                    title = ctx[0]\n",
        "                    if dataset_name.startswith('hotpotqa'):\n",
        "                        text = ''.join(ctx[1])\n",
        "                    else:\n",
        "                        text = ' '.join(ctx[1]) if isinstance(ctx[1], list) else ctx[1]\n",
        "                    needed_paragraphs.add(text)\n",
        "    \n",
        "    # corpus에서 해당 문서들 수집\n",
        "    docs = []\n",
        "    for doc in corpus:\n",
        "        if doc.get('text') in needed_paragraphs:\n",
        "            docs.append(f\"{doc['title']}\\n{doc['text']}\")\n",
        "    \n",
        "    return list(set(docs))\n",
        "\n",
        "\n",
        "def compute_doc_hash(doc: str, prefix: str = \"chunk-\") -> str:\n",
        "    \"\"\"문서의 해시값 계산\"\"\"\n",
        "    doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()\n",
        "    return f\"{prefix}{doc_hash}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "548b5f29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. OpenIE File Generation Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "Step별 OpenIE 파일 생성 함수\n",
        "- 전체 OpenIE 결과에서 해당 Step에 필요한 문서만 추출하여 저장\n",
        "\"\"\"\n",
        "\n",
        "def create_step_openie_files(\n",
        "    config: DatasetConfig,\n",
        "    samples: List[Dict],\n",
        "    corpus: List[Dict],\n",
        "    openie_data: Dict,\n",
        "    steps: List[int] = None,\n",
        "    llm_name: str = None\n",
        ") -> Dict[int, str]:\n",
        "    \"\"\"\n",
        "    Step별 OpenIE 파일 생성\n",
        "    \n",
        "    Args:\n",
        "        config: 데이터셋 설정\n",
        "        samples: 전체 샘플 리스트\n",
        "        corpus: 전체 코퍼스\n",
        "        openie_data: 전체 OpenIE 결과\n",
        "        steps: Step 리스트\n",
        "        llm_name: LLM 이름 (파일명용)\n",
        "    \n",
        "    Returns:\n",
        "        {step: openie_file_path} 딕셔너리\n",
        "    \"\"\"\n",
        "    steps = steps or STEPS\n",
        "    llm_name = llm_name or LLM_NAME\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[{config.name}] Step별 OpenIE 파일 생성\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # OpenIE 결과를 idx로 인덱싱\n",
        "    openie_docs = openie_data.get('docs', [])\n",
        "    openie_by_idx = {}\n",
        "    openie_by_passage = {}\n",
        "    for doc in openie_docs:\n",
        "        idx = doc.get('idx', '')\n",
        "        passage = doc.get('passage', '')\n",
        "        if idx:\n",
        "            openie_by_idx[idx] = doc\n",
        "        if passage:\n",
        "            openie_by_passage[passage] = doc\n",
        "    \n",
        "    print(f\"  전체 OpenIE 문서 수: {len(openie_docs)}\")\n",
        "    \n",
        "    step_openie_paths = {}\n",
        "    prev_docs = set()\n",
        "    \n",
        "    for step in steps:\n",
        "        print(f\"\\n--- Step {step} ---\")\n",
        "        \n",
        "        # 해당 Step까지의 문서 수집\n",
        "        step_docs = collect_paragraphs_for_step(samples, corpus, step, config.name)\n",
        "        print(f\"  수집된 문서 수: {len(step_docs)}\")\n",
        "        \n",
        "        # OpenIE 결과 필터링\n",
        "        step_openie_results = []\n",
        "        found_by_hash = 0\n",
        "        found_by_passage = 0\n",
        "        missing_docs = []\n",
        "        \n",
        "        for doc in step_docs:\n",
        "            doc_hash = compute_doc_hash(doc)\n",
        "            \n",
        "            if doc_hash in openie_by_idx:\n",
        "                step_openie_results.append(openie_by_idx[doc_hash])\n",
        "                found_by_hash += 1\n",
        "            elif doc in openie_by_passage:\n",
        "                step_openie_results.append(openie_by_passage[doc])\n",
        "                found_by_passage += 1\n",
        "            else:\n",
        "                missing_docs.append(doc[:100] + \"...\")\n",
        "        \n",
        "        print(f\"  찾은 OpenIE 결과: {len(step_openie_results)} (hash: {found_by_hash}, passage: {found_by_passage})\")\n",
        "        print(f\"  누락된 문서 수: {len(missing_docs)}\")\n",
        "        \n",
        "        if missing_docs and len(missing_docs) <= 3:\n",
        "            for m in missing_docs:\n",
        "                print(f\"    - {m}\")\n",
        "        \n",
        "        # Step 디렉토리 생성 및 저장\n",
        "        step_dir = f\"{config.output_dir}/step_{step}\"\n",
        "        os.makedirs(step_dir, exist_ok=True)\n",
        "        \n",
        "        openie_filename = f\"openie_results_ner_{llm_name.replace('/', '_')}.json\"\n",
        "        openie_dst_path = os.path.join(step_dir, openie_filename)\n",
        "        \n",
        "        openie_save = {\n",
        "            \"docs\": step_openie_results,\n",
        "            \"avg_ent_chars\": openie_data.get(\"avg_ent_chars\", 0),\n",
        "            \"avg_ent_words\": openie_data.get(\"avg_ent_words\", 0)\n",
        "        }\n",
        "        \n",
        "        save_json(openie_save, openie_dst_path)\n",
        "        step_openie_paths[step] = openie_dst_path\n",
        "        \n",
        "        # 통계\n",
        "        total_entities = sum(len(doc.get('extracted_entities', [])) for doc in step_openie_results)\n",
        "        total_triples = sum(len(doc.get('extracted_triples', [])) for doc in step_openie_results)\n",
        "        \n",
        "        print(f\"  ✓ 저장 완료: {openie_dst_path}\")\n",
        "        print(f\"    - 문서: {len(step_openie_results)}, 엔티티: {total_entities}, 트리플: {total_triples}\")\n",
        "        \n",
        "        if prev_docs:\n",
        "            new_docs = set(step_docs) - prev_docs\n",
        "            print(f\"    - 이전 Step 대비 추가 문서: {len(new_docs)}\")\n",
        "        \n",
        "        prev_docs = set(step_docs)\n",
        "    \n",
        "    return step_openie_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4185644e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. Graph Building Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "Step별 임베딩 및 그래프 구축 함수\n",
        "- Continual Learning 방식: 이전 Step의 그래프를 로드하여 확장\n",
        "\"\"\"\n",
        "\n",
        "def build_step_graphs(\n",
        "    config: DatasetConfig,\n",
        "    samples: List[Dict],\n",
        "    corpus: List[Dict],\n",
        "    steps: List[int] = None,\n",
        "    llm_name: str = None,\n",
        "    embedding_model_name: str = None,\n",
        "    force_rebuild: bool = False\n",
        ") -> Dict[int, str]:\n",
        "    \"\"\"\n",
        "    Step별 그래프 구축 (Continual Learning 방식)\n",
        "    \n",
        "    Step 250: 새로 시작\n",
        "    Step 500+: 이전 Step의 그래프를 로드하여 확장\n",
        "    \n",
        "    Args:\n",
        "        config: 데이터셋 설정\n",
        "        samples: 전체 샘플 리스트\n",
        "        corpus: 전체 코퍼스\n",
        "        steps: Step 리스트\n",
        "        llm_name: LLM 이름\n",
        "        embedding_model_name: 임베딩 모델 이름\n",
        "        force_rebuild: 기존 그래프 무시하고 재구축\n",
        "    \n",
        "    Returns:\n",
        "        {step: graph_dir_path} 딕셔너리\n",
        "    \"\"\"\n",
        "    steps = steps or STEPS\n",
        "    llm_name = llm_name or LLM_NAME\n",
        "    embedding_model_name = embedding_model_name or EMBEDDING_MODEL_NAME\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[{config.name}] Step별 그래프 구축\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    step_graph_paths = {}\n",
        "    prev_step = None\n",
        "    \n",
        "    for i, step in enumerate(steps):\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Step {step} 시작\")\n",
        "        print(f\"{'='*40}\")\n",
        "        \n",
        "        # 해당 Step까지의 문서 수집\n",
        "        step_docs = collect_paragraphs_for_step(samples, corpus, step, config.name)\n",
        "        print(f\"  인덱싱할 문서 수: {len(step_docs)}\")\n",
        "        \n",
        "        # Step 디렉토리 설정\n",
        "        step_dir = f\"{config.output_dir}/step_{step}\"\n",
        "        os.makedirs(step_dir, exist_ok=True)\n",
        "        \n",
        "        # 그래프 경로 확인\n",
        "        model_dir_name = f\"{llm_name.replace('/', '_')}_{embedding_model_name.replace('/', '_')}\"\n",
        "        graph_path = os.path.join(step_dir, model_dir_name, \"graph.pickle\")\n",
        "        \n",
        "        # 이미 그래프가 있으면 스킵 (force_rebuild=False일 때)\n",
        "        if os.path.exists(graph_path) and not force_rebuild:\n",
        "            print(f\"  ✓ 기존 그래프 존재, 스킵: {graph_path}\")\n",
        "            step_graph_paths[step] = step_dir\n",
        "            prev_step = step\n",
        "            continue\n",
        "        \n",
        "        # HippoRAG 설정\n",
        "        base_config = BaseConfig()\n",
        "        base_config.save_dir = step_dir\n",
        "        base_config.force_index_from_scratch = False\n",
        "        base_config.force_openie_from_scratch = False\n",
        "        base_config.embedding_batch_size = 16\n",
        "        \n",
        "        hipporag = HippoRAG(\n",
        "            global_config=base_config,\n",
        "            save_dir=step_dir,\n",
        "            llm_model_name=llm_name,\n",
        "            embedding_model_name=embedding_model_name\n",
        "        )\n",
        "        \n",
        "        # Continual Learning: 이전 Step 그래프 로드 (첫 Step 제외)\n",
        "        if i > 0 and prev_step is not None:\n",
        "            prev_graph_path = os.path.join(\n",
        "                f\"{config.output_dir}/step_{prev_step}\",\n",
        "                model_dir_name,\n",
        "                \"graph.pickle\"\n",
        "            )\n",
        "            \n",
        "            if os.path.exists(prev_graph_path):\n",
        "                print(f\"  이전 그래프 로드 중: {prev_graph_path}\")\n",
        "                try:\n",
        "                    with open(prev_graph_path, 'rb') as f:\n",
        "                        hipporag.graph = pickle.load(f)\n",
        "                    print(f\"  ✓ 이전 그래프 로드 완료\")\n",
        "                    print(f\"    - 노드 수: {hipporag.graph.vcount()}\")\n",
        "                    print(f\"    - 엣지 수: {hipporag.graph.ecount()}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ⚠ 이전 그래프 로드 실패: {e}\")\n",
        "                    print(f\"  → 새로 시작\")\n",
        "            else:\n",
        "                print(f\"  ⚠ 이전 그래프 없음, 새로 시작\")\n",
        "        else:\n",
        "            print(f\"  첫 Step, 새로 시작\")\n",
        "        \n",
        "        # 인덱싱 실행\n",
        "        print(f\"  인덱싱 시작...\")\n",
        "        hipporag.index(step_docs)\n",
        "        \n",
        "        # 그래프 정보 출력\n",
        "        if hasattr(hipporag, 'graph') and hipporag.graph is not None:\n",
        "            print(f\"  ✓ 인덱싱 완료\")\n",
        "            print(f\"    - 노드 수: {hipporag.graph.vcount()}\")\n",
        "            print(f\"    - 엣지 수: {hipporag.graph.ecount()}\")\n",
        "        \n",
        "        step_graph_paths[step] = step_dir\n",
        "        prev_step = step\n",
        "        \n",
        "        # 메모리 정리\n",
        "        del hipporag\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return step_graph_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7489e69d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 메인 파이프라인 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 6. Main Pipeline Function\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "전체 파이프라인 실행 함수\n",
        "1. OpenIE 파일 생성\n",
        "2. Step별 그래프 구축\n",
        "\"\"\"\n",
        "\n",
        "def run_dataset_pipeline(\n",
        "    dataset_name: str,\n",
        "    steps: List[int] = None,\n",
        "    force_rebuild: bool = False,\n",
        "    skip_openie: bool = False,\n",
        "    skip_graph: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    데이터셋에 대한 전체 파이프라인 실행\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: 데이터셋 이름 (hotpotqa, 2wikimultihopqa)\n",
        "        steps: Step 리스트\n",
        "        force_rebuild: 기존 그래프 무시하고 재구축\n",
        "        skip_openie: OpenIE 파일 생성 스킵\n",
        "        skip_graph: 그래프 구축 스킵\n",
        "    \n",
        "    Returns:\n",
        "        결과 딕셔너리\n",
        "    \"\"\"\n",
        "    steps = steps or STEPS\n",
        "    \n",
        "    if dataset_name not in DATASETS:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(DATASETS.keys())}\")\n",
        "    \n",
        "    config = DATASETS[dataset_name]\n",
        "    \n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"# Dataset: {dataset_name}\")\n",
        "    print(f\"# Steps: {steps}\")\n",
        "    print(f\"{'#'*60}\")\n",
        "    \n",
        "    # 1. 데이터 로드\n",
        "    print(f\"\\n[1] 데이터 로드 중...\")\n",
        "    \n",
        "    if not os.path.exists(config.samples_path):\n",
        "        print(f\"  ⚠ 샘플 파일 없음: {config.samples_path}\")\n",
        "        return {\"error\": f\"Samples file not found: {config.samples_path}\"}\n",
        "    \n",
        "    if not os.path.exists(config.corpus_path):\n",
        "        print(f\"  ⚠ 코퍼스 파일 없음: {config.corpus_path}\")\n",
        "        return {\"error\": f\"Corpus file not found: {config.corpus_path}\"}\n",
        "    \n",
        "    samples = load_json(config.samples_path)\n",
        "    corpus = load_json(config.corpus_path)\n",
        "    \n",
        "    print(f\"  ✓ 샘플 수: {len(samples)}\")\n",
        "    print(f\"  ✓ 코퍼스 문서 수: {len(corpus)}\")\n",
        "    \n",
        "    # 2. OpenIE 데이터 로드 및 파일 생성\n",
        "    openie_paths = {}\n",
        "    if not skip_openie:\n",
        "        print(f\"\\n[2] OpenIE 파일 생성 중...\")\n",
        "        \n",
        "        if not os.path.exists(config.openie_path):\n",
        "            print(f\"  ⚠ OpenIE 파일 없음: {config.openie_path}\")\n",
        "            print(f\"  → OpenIE 파일 생성 스킵\")\n",
        "        else:\n",
        "            openie_data = load_json(config.openie_path)\n",
        "            print(f\"  ✓ OpenIE 데이터 로드 완료: {len(openie_data.get('docs', []))} 문서\")\n",
        "            \n",
        "            openie_paths = create_step_openie_files(\n",
        "                config=config,\n",
        "                samples=samples,\n",
        "                corpus=corpus,\n",
        "                openie_data=openie_data,\n",
        "                steps=steps\n",
        "            )\n",
        "    else:\n",
        "        print(f\"\\n[2] OpenIE 파일 생성 스킵\")\n",
        "    \n",
        "    # 3. 그래프 구축\n",
        "    graph_paths = {}\n",
        "    if not skip_graph:\n",
        "        print(f\"\\n[3] 그래프 구축 중...\")\n",
        "        \n",
        "        graph_paths = build_step_graphs(\n",
        "            config=config,\n",
        "            samples=samples,\n",
        "            corpus=corpus,\n",
        "            steps=steps,\n",
        "            force_rebuild=force_rebuild\n",
        "        )\n",
        "    else:\n",
        "        print(f\"\\n[3] 그래프 구축 스킵\")\n",
        "    \n",
        "    # 결과 요약\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[{dataset_name}] 완료!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"steps\": steps,\n",
        "        \"openie_paths\": openie_paths,\n",
        "        \"graph_paths\": graph_paths,\n",
        "        \"output_dir\": config.output_dir,\n",
        "    }\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def run_all_datasets(\n",
        "    steps: List[int] = None,\n",
        "    force_rebuild: bool = False,\n",
        "    skip_openie: bool = False,\n",
        "    skip_graph: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    모든 데이터셋에 대해 파이프라인 실행\n",
        "    \n",
        "    Returns:\n",
        "        {dataset_name: result} 딕셔너리\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for dataset_name in DATASETS.keys():\n",
        "        result = run_dataset_pipeline(\n",
        "            dataset_name=dataset_name,\n",
        "            steps=steps,\n",
        "            force_rebuild=force_rebuild,\n",
        "            skip_openie=skip_openie,\n",
        "            skip_graph=skip_graph\n",
        "        )\n",
        "        results[dataset_name] = result\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aca423a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 실행 섹션\n",
        "\n",
        "## 1. HotpotQA 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86fa1339",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. Run HotpotQA Pipeline\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "HotpotQA 데이터셋 파이프라인 실행\n",
        "\n",
        "출력 경로: _hippo_rag_MHQA_CL/hotpotqa/step_{250,500,750,1000}/\n",
        "\"\"\"\n",
        "\n",
        "# HotpotQA 실행\n",
        "hotpotqa_result = run_dataset_pipeline(\n",
        "    dataset_name=\"hotpotqa\",\n",
        "    steps=STEPS,\n",
        "    force_rebuild=False,  # True로 하면 기존 그래프 무시하고 재구축\n",
        "    skip_openie=False,    # OpenIE 파일 이미 있으면 True\n",
        "    skip_graph=False      # 그래프만 스킵하려면 True\n",
        ")\n",
        "\n",
        "print(\"\\n결과:\")\n",
        "print(f\"  Output Dir: {hotpotqa_result.get('output_dir')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb848aa8",
      "metadata": {},
      "source": [
        "## 2. 2WikiMultiHopQA 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99ec683",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. Run 2WikiMultiHopQA Pipeline\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "2WikiMultiHopQA 데이터셋 파이프라인 실행\n",
        "\n",
        "출력 경로: _hippo_rag_MHQA_CL/2wikimultihopqa/step_{250,500,750,1000}/\n",
        "\"\"\"\n",
        "\n",
        "# 2WikiMultiHopQA 실행\n",
        "wiki2_result = run_dataset_pipeline(\n",
        "    dataset_name=\"2wikimultihopqa\",\n",
        "    steps=STEPS,\n",
        "    force_rebuild=False,\n",
        "    skip_openie=False,\n",
        "    skip_graph=False\n",
        ")\n",
        "\n",
        "print(\"\\n결과:\")\n",
        "print(f\"  Output Dir: {wiki2_result.get('output_dir')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hipporagenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
