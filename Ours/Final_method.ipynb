{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8c543b3",
      "metadata": {},
      "source": [
        "# Joint Method: Continual Learning with QCEdge\n",
        "\n",
        "## 방법론 개요\n",
        "Multi-hop 질문에 대해 Query Decomposition과 Knowledge Graph 기반 검색을 결합하여,  \n",
        "이전 검색 경험(QCEdge)을 활용해 점진적으로 검색 성능을 개선하는 Continual Learning 방식\n",
        "\n",
        "### 파이프라인 구조\n",
        "1. **Step 1 (Step 250)**: 초기 검색 + QCEdge 추출\n",
        "2. **Step 2 (Step 500)**: QCEdge로 그래프 강화 + Intersection QCEdge 계산\n",
        "3. **Step 3 (Step 750)**: Intersection + Extra QCEdge로 그래프 강화 (`theta_mult=15`, `wub=6`)\n",
        "4. **Step 4 (Step 1000)**: 더 강한 강화 (`theta_mult=30`, `wub=20`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f4e1d18a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. Imports & Configuration\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "필수 라이브러리 및 HippoRAG 모듈 임포트\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# HippoRAG 경로 설정\n",
        "sys.path.append(\"/NAS/minyeol/HippoRAG/src\")\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
        "\n",
        "from hipporag.HippoRAG import HippoRAG\n",
        "from hipporag.llm.openai_gpt import CacheOpenAI\n",
        "from hipporag.rerank import DSPyFilter\n",
        "from hipporag.utils.config_utils import BaseConfig\n",
        "from hipporag.utils.llm_utils import filter_invalid_triples\n",
        "from hipporag.utils.misc_utils import min_max_normalize, text_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f3b5571d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 설정 완료\n",
            "  - Datasets: ['musique']\n",
            "  - Output: /NAS/minyeol/HippoRAG/Ours/outputs/final_method_outputs\n",
            "  - Steps: [250, 500, 750, 1000]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. Configuration & Constants\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "실험 설정값 및 상수 정의\n",
        "- 파일 경로\n",
        "- Step별 파라미터\n",
        "- 프롬프트 설정\n",
        "if __name__ == \"__main__\":\n",
        "    STEPS = [250, 500, 750]\n",
        "    MAX_SAMPLES = 250\n",
        "    TOP_K_PER_HOP = 5\n",
        "    TOP_K_PER_BRIDGE = 5\n",
        "    TOP_K_EDGES = 30\n",
        "    THETA = 15\n",
        "    WEIGHT_UPPER_BOUND = 3  # Step 500용 (고정)\n",
        "    RESET_PROB = 0.85\n",
        "\"\"\"\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"파이프라인 전체 설정\"\"\"\n",
        "    # 경로 설정\n",
        "    # 데이터셋 설정 (여러 데이터셋 지원)\n",
        "    dataset_names: List[str] = field(default_factory=lambda: [\"musique\"])  # [\"musique\", \"hotpotqa\", \"2wikimultihopqa\"]\n",
        "    \n",
        "    # 경로 설정 (dataset_name에 따라 동적으로 설정됨)\n",
        "    dataset_path: str = None  # dataset_name에 따라 자동 설정\n",
        "    prompt_root: str = \"/NAS/minyeol/HippoRAG/Ours/prompts/QD_bridge2_prompts_reasoning\"\n",
        "    output_dir: str = \"/NAS/minyeol/HippoRAG/Ours/outputs/final_method_outputs\"\n",
        "    hippo_base_dir: str = \"/NAS/minyeol/HippoRAG/Ours/_hippo_rag_MHQA_CL\"  # 데이터셋별 하위 디렉토리 사용\n",
        "    \n",
        "    # Step 값\n",
        "    step_values: List[int] = field(default_factory=lambda: [250, 500, 750, 1000])\n",
        "    \n",
        "    # Step 1 파라미터 (Query Decomposition & Initial Retrieval)\n",
        "    top_k_per_bridge: int = 5       # Bridge question당 검색할 triple 수\n",
        "    max_bridge_triples: int = 10    # 최대 bridge triple 수\n",
        "    top_k_per_hop: int = 5         # Sub-question당 검색할 triple 수\n",
        "    aggregation_method: str = \"sum\" # Fact 점수 집계: \"sum\" | \"weighted_sum\" | \"max\"\n",
        "    \n",
        "    # Step 1 STPPR 파라미터\n",
        "    top_k_passages_stppr: int = 5  # STPPR 실행할 passage 수\n",
        "    top_k_edges: int = 30           # 저장할 QCEdge 수\n",
        "    rbs_alpha: float = 0.5          # RBS backward search reset probability\n",
        "    rbs_eps: float = 1e-5           # RBS 수렴 threshold\n",
        "    \n",
        "    # Step 2 파라미터 (Edge Strengthening with QCEdge)\n",
        "    theta_step2: float = 15.0       # Edge 강화 강도\n",
        "    wub_step2: float = 3.0          # Weight upper bound\n",
        "    \n",
        "    # Step 3 파라미터 (Intersection + Extra QCEdge) - Step 750\n",
        "    percentile_step3: float = 50.0  # Extra QCEdge 선택 percentile\n",
        "    theta_step3: float = 15.0       # 기본 theta\n",
        "    theta_mult_step3: float = 15.0  # theta multiplier (최종: theta * mult)\n",
        "    wub_step3: float = 6.0          # Weight upper bound\n",
        "    \n",
        "    # Step 4 파라미터 - Step 1000\n",
        "    percentile_step4: float = 50.0\n",
        "    theta_step4: float = 15.0\n",
        "    theta_mult_step4: float = 30.0  # 더 강한 강화\n",
        "    wub_step4: float = 20.0         # 더 높은 upper bound\n",
        "    \n",
        "    # 샘플 수 제한\n",
        "    max_samples: int = 250\n",
        "\n",
        "    \n",
        "    def get_dataset_path(self, dataset_name: str) -> str:\n",
        "        \"\"\"데이터셋 이름으로 경로 생성\"\"\"\n",
        "        return f\"/NAS/minyeol/HippoRAG/reproduce/dataset/{dataset_name}.json\"\n",
        "    \n",
        "    def get_hippo_base_dir(self, dataset_name: str) -> str:\n",
        "        \"\"\"데이터셋별 hippo base 디렉토리 경로 생성\"\"\"\n",
        "        return f\"{self.hippo_base_dir}/{dataset_name}\"\n",
        "    \n",
        "    def get_output_dir(self, dataset_name: str) -> str:\n",
        "        \"\"\"데이터셋별 output 디렉토리 경로 생성\"\"\"\n",
        "        return f\"{self.output_dir}/{dataset_name}\"\n",
        "\n",
        "\n",
        "# 프롬프트 파일 매핑\n",
        "PROMPT_FILES = {\n",
        "    \"atomic\": \"birdge_extraction_with_description.txt\",  # Atomic Bridge Question 추출용\n",
        "    \"context\": \"simple_query_decomposition.txt\",         # Context-aware Query Decomposition용\n",
        "}\n",
        "\n",
        "# 전역 설정 인스턴스\n",
        "CONFIG = PipelineConfig()\n",
        "\n",
        "print(f\"✓ 설정 완료\")\n",
        "print(f\"  - Datasets: {CONFIG.dataset_names}\")\n",
        "print(f\"  - Output: {CONFIG.output_dir}\")\n",
        "print(f\"  - Steps: {CONFIG.step_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fd1283eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 데이터 클래스 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 3. Data Classes\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "파이프라인에서 사용하는 데이터 구조 정의\n",
        "\"\"\"\n",
        "\n",
        "@dataclass\n",
        "class AtomicBridgeQuestion:\n",
        "    \"\"\"\n",
        "    Atomic Bridge Question 구조\n",
        "    \n",
        "    Multi-hop 질문에서 추출된 단일 factual question과 메타 정보\n",
        "    \n",
        "    Attributes:\n",
        "        question: 추출된 atomic factual question\n",
        "        description: 해당 질문의 역할 설명 (Role_in_chain)\n",
        "        info_gain: 이 질문을 해결하면 얻는 정보 (Info_obtained)\n",
        "        follow_up: 이후 해결해야 할 문제들 (Remaining_subproblems)\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    description: str\n",
        "    info_gain: str\n",
        "    follow_up: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HopInfo:\n",
        "    \"\"\"\n",
        "    각 Hop(Sub-question)의 검색 결과 정보\n",
        "    \n",
        "    Attributes:\n",
        "        hop: hop 인덱스 (0-based)\n",
        "        sub_question: 해당 hop의 sub-question\n",
        "        top_facts_before_filter: LLM 필터 전 top facts\n",
        "        top_scores_before_filter: LLM 필터 전 scores\n",
        "        filtered_facts: LLM 필터 후 facts\n",
        "        filtered_scores: LLM 필터 후 scores\n",
        "    \"\"\"\n",
        "    hop: int\n",
        "    sub_question: str\n",
        "    top_facts_before_filter: List[Tuple[str, str, str]]\n",
        "    top_scores_before_filter: List[float]\n",
        "    filtered_facts: List[Tuple[str, str, str]]\n",
        "    filtered_scores: List[float]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QCEdgeEntry:\n",
        "    \"\"\"\n",
        "    Query-Conditioned Edge 정보\n",
        "    \n",
        "    STPPR을 통해 추출된 query와 연관된 중요 edge\n",
        "    \n",
        "    Attributes:\n",
        "        edge: (source_node_id, target_node_id) 튜플\n",
        "        importance: 정규화된 중요도 (0~1)\n",
        "        qc_edge_value: PPR forward score × RBS backward flow\n",
        "        step: 추출된 step 번호\n",
        "    \"\"\"\n",
        "    edge: Tuple[int, int]\n",
        "    importance: float\n",
        "    qc_edge_value: float\n",
        "    step: str = \"unknown\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    \"\"\"\n",
        "    검색 결과 구조\n",
        "    \n",
        "    Attributes:\n",
        "        passages: 검색된 passage 내용 리스트\n",
        "        scores: 각 passage의 점수\n",
        "        recall: Recall@K 점수\n",
        "        hit: Hit@K (1 or 0)\n",
        "        fallback_used: Dense retrieval fallback 사용 여부\n",
        "    \"\"\"\n",
        "    passages: List[str]\n",
        "    scores: List[float]\n",
        "    recall: float\n",
        "    hit: int\n",
        "    fallback_used: bool\n",
        "\n",
        "\n",
        "print(\"✓ 데이터 클래스 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "697c7cb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 유틸리티 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 4. Utility Functions - Data Loading & Saving\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "데이터 로딩, 저장, HippoRAG 빌드 등 기본 유틸리티 함수\n",
        "\"\"\"\n",
        "\n",
        "def load_samples(dataset_name: str, path: str = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    데이터셋 로드 (musique, hotpotqa, 2wikimultihopqa 지원)\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: 데이터셋 이름 (musique, hotpotqa, 2wikimultihopqa)\n",
        "        path: 데이터셋 JSON 파일 경로 (None이면 CONFIG 사용)\n",
        "    \n",
        "    Returns:\n",
        "        샘플 리스트 (각 샘플은 question, context, answer 등 포함)\n",
        "    \"\"\"\n",
        "    path = path or CONFIG.get_dataset_path(dataset_name)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def get_queries(samples: List[Dict[str, Any]]) -> List[str]:\n",
        "    \"\"\"샘플에서 질문만 추출\"\"\"\n",
        "    return [sample[\"question\"] for sample in samples]\n",
        "\n",
        "\n",
        "def get_gold_docs(samples: List[Dict[str, Any]], dataset_name: str = None) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    샘플에서 정답 문서(supporting facts) 추출 (데이터셋별 처리)\n",
        "    \n",
        "    Returns:\n",
        "        각 샘플별 정답 문서 리스트의 리스트\n",
        "    \"\"\"\n",
        "    gold_docs: List[List[str]] = []\n",
        "    for sample in samples:\n",
        "        if 'supporting_facts' in sample:  # hotpotqa, 2wikimultihopqa\n",
        "            gold_title = set([item[0] for item in sample['supporting_facts']])\n",
        "            gold_title_and_content_list = [item for item in sample['context'] if item[0] in gold_title]\n",
        "            if dataset_name and dataset_name.startswith('hotpotqa'):\n",
        "                gold_doc = [item[0] + '\\n' + ''.join(item[1]) for item in gold_title_and_content_list]\n",
        "            else:\n",
        "                gold_doc = [item[0] + '\\n' + ' '.join(item[1]) for item in gold_title_and_content_list]\n",
        "        elif 'contexts' in sample:\n",
        "            gold_doc = [item['title'] + '\\n' + item['text'] for item in sample['contexts'] if item['is_supporting']]\n",
        "        else:\n",
        "            assert 'paragraphs' in sample, \"`paragraphs` should be in sample, or consider the setting not to evaluate retrieval\"\n",
        "            gold_paragraphs = []\n",
        "            for item in sample['paragraphs']:\n",
        "                if 'is_supporting' in item and item['is_supporting'] is False:\n",
        "                    continue\n",
        "                gold_paragraphs.append(item)\n",
        "            gold_doc = [item['title'] + '\\n' + (item['text'] if 'text' in item else item.get('paragraph_text', '')) for item in gold_paragraphs]\n",
        "        gold_doc = list(set(gold_doc))\n",
        "        gold_docs.append(gold_doc)\n",
        "    return gold_docs\n",
        "\n",
        "def build_hipporag(step: int, dataset_name: str, config: PipelineConfig = None) -> HippoRAG:\n",
        "    \"\"\"\n",
        "    특정 Step의 HippoRAG 객체 빌드\n",
        "    \n",
        "    Args:\n",
        "        step: step 번호 (250, 500, 750, 1000)\n",
        "        config: 파이프라인 설정\n",
        "    \n",
        "    Returns:\n",
        "        초기화된 HippoRAG 객체\n",
        "    \"\"\"\n",
        "    config = config or CONFIG\n",
        "    hippo_base = config.get_hippo_base_dir(dataset_name)\n",
        "    save_dir = f\"{hippo_base}/step_{step}\"\n",
        "    \n",
        "    cfg = BaseConfig(\n",
        "        save_dir=save_dir,\n",
        "        llm_base_url=\"https://api.openai.com/v1\",\n",
        "        llm_name=\"gpt-4o-mini\",\n",
        "        dataset=\"adhoc\",\n",
        "        embedding_model_name=\"nvidia/NV-Embed-v2\",\n",
        "        force_index_from_scratch=False,\n",
        "        force_openie_from_scratch=False,\n",
        "    )\n",
        "    hippo = HippoRAG(global_config=cfg)\n",
        "    hippo.prepare_retrieval_objects()\n",
        "    return hippo\n",
        "\n",
        "\n",
        "def save_json(data: Any, path: str) -> None:\n",
        "    \"\"\"JSON 파일로 저장\"\"\"\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"  → 저장 완료: {path}\")\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Any:\n",
        "    \"\"\"JSON 파일 로드\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "print(\"✓ 유틸리티 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "712acb6f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LLM 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 5. LLM Functions - Prompt & Parsing\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "LLM 호출 및 응답 파싱 함수\n",
        "- Atomic Bridge Question 추출\n",
        "- Query Decomposition\n",
        "\"\"\"\n",
        "\n",
        "def _load_prompt(name: str) -> str:\n",
        "    \"\"\"프롬프트 파일 로드\"\"\"\n",
        "    path = Path(CONFIG.prompt_root) / name\n",
        "    if not path.is_file():\n",
        "        raise FileNotFoundError(f\"프롬프트 파일 없음: {path}\")\n",
        "    return path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def _fill_query_placeholders(template: str, question: str) -> str:\n",
        "    \"\"\"프롬프트 템플릿의 플레이스홀더를 실제 질문으로 대체\"\"\"\n",
        "    for placeholder in (\"<<ORIGINAL_QUERY>>\", \"<<YOUR_MULTI_HOP_QUESTION_HERE>>\"):\n",
        "        template = template.replace(placeholder, question.strip())\n",
        "    return template\n",
        "\n",
        "\n",
        "def _coerce_followups(value: Any) -> List[str]:\n",
        "    \"\"\"follow_up 값을 리스트로 변환\"\"\"\n",
        "    if isinstance(value, list):\n",
        "        return [str(v).strip() for v in value if str(v).strip()]\n",
        "    if isinstance(value, str):\n",
        "        return [seg.strip() for seg in re.split(r\"[;\\n•\\-]+\", value) if seg.strip()]\n",
        "    return []\n",
        "\n",
        "\n",
        "def _parse_atomic_output(text: str) -> List[AtomicBridgeQuestion]:\n",
        "    \"\"\"\n",
        "    LLM 응답에서 Atomic Bridge Questions 파싱\n",
        "    \n",
        "    여러 출력 형식 지원:\n",
        "    1. {\"bridge_questions\": [...], \"descriptions\": [...]} 형식\n",
        "    2. [{\"question\": ..., \"description\": ...}, ...] 형식\n",
        "    3. 텍스트 형식 (정규식 파싱)\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    \n",
        "    # JSON 파싱 시도\n",
        "    try:\n",
        "        payload = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        payload = None\n",
        "    \n",
        "    # 형식 1: bridge_questions + descriptions\n",
        "    if isinstance(payload, dict) and \"bridge_questions\" in payload and \"descriptions\" in payload:\n",
        "        items = []\n",
        "        for idx, question in enumerate(payload[\"bridge_questions\"]):\n",
        "            desc = payload[\"descriptions\"][idx] if idx < len(payload[\"descriptions\"]) else {}\n",
        "            items.append(AtomicBridgeQuestion(\n",
        "                question=str(question).strip(),\n",
        "                description=str(desc.get(\"Role_in_chain\", \"\")).strip(),\n",
        "                info_gain=str(desc.get(\"Info_obtained\", \"\")).strip(),\n",
        "                follow_up=[str(x).strip() for x in desc.get(\"Remaining_subproblems\", []) if str(x).strip()],\n",
        "            ))\n",
        "        return items[:2]  # 최대 2개\n",
        "    \n",
        "    # 형식 2: 리스트 형식\n",
        "    if isinstance(payload, list):\n",
        "        items = []\n",
        "        for entry in payload:\n",
        "            q = entry.get(\"question\") or entry.get(\"atomic_question\")\n",
        "            if not q:\n",
        "                continue\n",
        "            items.append(AtomicBridgeQuestion(\n",
        "                question=str(q).strip(),\n",
        "                description=str(entry.get(\"description\", \"\")).strip(),\n",
        "                info_gain=str(entry.get(\"info_gain\", \"\")).strip(),\n",
        "                follow_up=_coerce_followups(entry.get(\"follow_up\") or entry.get(\"follow_up_tasks\")),\n",
        "            ))\n",
        "        if items:\n",
        "            return items[:2]\n",
        "    \n",
        "    # 형식 3: 텍스트 파싱\n",
        "    pattern = re.compile(\n",
        "        r\"\\d+\\.\\s*Question\\s*:\\s*(?P<q>.+?)\\s*(Description\\s*:\\s*(?P<d>.+?))?\\s*\"\n",
        "        r\"(Info\\s*Gain\\s*:\\s*(?P<i>.+?))?\\s*(Follow[-\\s]*Up\\s*:\\s*(?P<f>.+?))?(?=\\n\\d+\\.|$)\",\n",
        "        re.IGNORECASE | re.DOTALL,\n",
        "    )\n",
        "    items = []\n",
        "    for match in pattern.finditer(text):\n",
        "        items.append(AtomicBridgeQuestion(\n",
        "            question=match.group(\"q\").strip(),\n",
        "            description=(match.group(\"d\") or \"\").strip(),\n",
        "            info_gain=(match.group(\"i\") or \"\").strip(),\n",
        "            follow_up=_coerce_followups(match.group(\"f\")),\n",
        "        ))\n",
        "    return items[:2]\n",
        "\n",
        "\n",
        "def extract_atomic_bridge_questions(\n",
        "    question: str, \n",
        "    base_cfg: BaseConfig, \n",
        "    max_tokens: int = 512\n",
        ") -> List[AtomicBridgeQuestion]:\n",
        "    \"\"\"\n",
        "    Step 1.1: Multi-hop 질문에서 Atomic Bridge Questions 추출\n",
        "    \n",
        "    LLM을 사용하여 복잡한 multi-hop 질문을 1~2개의 단순한 factual question으로 분해\n",
        "    각 question에 대해 description, info_gain, follow_up 정보도 함께 추출\n",
        "    \n",
        "    Args:\n",
        "        question: 원본 multi-hop 질문\n",
        "        base_cfg: HippoRAG BaseConfig\n",
        "        max_tokens: LLM 응답 최대 토큰\n",
        "    \n",
        "    Returns:\n",
        "        AtomicBridgeQuestion 리스트 (최대 2개)\n",
        "    \"\"\"\n",
        "    prompt = _fill_query_placeholders(_load_prompt(PROMPT_FILES[\"atomic\"]), question)\n",
        "    \n",
        "    llm = CacheOpenAI.from_experiment_config(\n",
        "        BaseConfig(\n",
        "            save_dir=os.path.join(base_cfg.save_dir, \"bridge_atomic\"),\n",
        "            llm_base_url=base_cfg.llm_base_url,\n",
        "            llm_name=base_cfg.llm_name,\n",
        "            dataset=\"adhoc\",\n",
        "            embedding_model_name=base_cfg.embedding_model_name,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    raw, *_ = llm.infer(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_completion_tokens=max_tokens,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    text = raw[0] if isinstance(raw, (list, tuple)) else raw or \"\"\n",
        "    return _parse_atomic_output(text)\n",
        "\n",
        "\n",
        "def build_context_bundle(\n",
        "    question: str, \n",
        "    atomic_items: List[AtomicBridgeQuestion], \n",
        "    triples: List[Tuple[str, str, str]]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Step 1.3 준비: Query Decomposition을 위한 컨텍스트 번들 생성\n",
        "    \n",
        "    Original Query + Atomic Questions + Retrieved Triples를\n",
        "    LLM에게 전달할 형식으로 조합\n",
        "    \n",
        "    Args:\n",
        "        question: 원본 질문\n",
        "        atomic_items: 추출된 atomic bridge questions\n",
        "        triples: 검색된 triples\n",
        "    \n",
        "    Returns:\n",
        "        포맷팅된 컨텍스트 문자열\n",
        "    \"\"\"\n",
        "    # Atomic Questions 섹션\n",
        "    atomic_section = []\n",
        "    for idx, item in enumerate(atomic_items, 1):\n",
        "        follow_up = \"; \".join(item.follow_up) if item.follow_up else \"N/A\"\n",
        "        atomic_section.append(\n",
        "            f\"{idx}. Question: {item.question}\\n\"\n",
        "            f\"   Description: {item.description or 'N/A'}\\n\"\n",
        "            f\"   Info_obtained: {item.info_gain or 'N/A'}\\n\"\n",
        "            f\"   Remaining_subproblems: {follow_up}\"\n",
        "        )\n",
        "    \n",
        "    # Triples 섹션\n",
        "    triple_section = [f\"{idx}. ({s}, {p}, {o})\" for idx, (s, p, o) in enumerate(triples, 1)]\n",
        "    \n",
        "    return \"\\n\".join([\n",
        "        \"[Original Query]\",\n",
        "        question.strip(),\n",
        "        \"[Atomic Questions]\",\n",
        "        \"\\n\".join(atomic_section) if atomic_section else \"No atomic questions extracted.\",\n",
        "        \"[Retrieved Triples]\",\n",
        "        \"\\n\".join(triple_section) if triple_section else \"No supporting triples retrieved.\",\n",
        "    ])\n",
        "\n",
        "\n",
        "def decompose_query(\n",
        "    question: str, \n",
        "    context_bundle: str, \n",
        "    base_cfg: BaseConfig, \n",
        "    max_tokens: int = 512\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Step 1.3: Context-aware Query Decomposition\n",
        "    \n",
        "    Original Query + Atomic Questions + Triples 정보를 활용하여\n",
        "    원본 질문을 최대한 단순한 sub-questions로 분해\n",
        "    \n",
        "    Args:\n",
        "        question: 원본 질문\n",
        "        context_bundle: build_context_bundle()로 생성된 컨텍스트\n",
        "        base_cfg: HippoRAG BaseConfig\n",
        "        max_tokens: LLM 응답 최대 토큰\n",
        "    \n",
        "    Returns:\n",
        "        Sub-questions 리스트\n",
        "    \"\"\"\n",
        "    prompt = _fill_query_placeholders(_load_prompt(PROMPT_FILES[\"context\"]), question)\n",
        "    prompt = prompt.replace(\"<<ATOMIC_AND_TRIPLES>>\", context_bundle)\n",
        "    \n",
        "    llm = CacheOpenAI.from_experiment_config(\n",
        "        BaseConfig(\n",
        "            save_dir=os.path.join(base_cfg.save_dir, \"bridge_context\"),\n",
        "            llm_base_url=base_cfg.llm_base_url,\n",
        "            llm_name=base_cfg.llm_name,\n",
        "            dataset=\"adhoc\",\n",
        "            embedding_model_name=base_cfg.embedding_model_name,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    raw, *_ = llm.infer(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_completion_tokens=max_tokens,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    text = raw[0] if isinstance(raw, (list, tuple)) else raw or \"\"\n",
        "    text = text.strip()\n",
        "    \n",
        "    # JSON 파싱 시도\n",
        "    try:\n",
        "        payload = json.loads(text)\n",
        "        if isinstance(payload, dict):\n",
        "            payload = payload.get(\"sub_questions\") or payload.get(\"subqs\")\n",
        "        if isinstance(payload, list):\n",
        "            return [str(item).strip() for item in payload if str(item).strip()]\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    \n",
        "    # 텍스트 파싱 (번호 형식)\n",
        "    pattern = re.compile(r\"^\\s*\\d+\\.\\s*(.+)\\s*$\")\n",
        "    lines = [\n",
        "        pattern.match(line).group(1) if pattern.match(line) else line.strip() \n",
        "        for line in text.splitlines()\n",
        "    ]\n",
        "    return [line for line in lines if line]\n",
        "\n",
        "\n",
        "print(\"✓ LLM 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b360dd66",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Triple 검색 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 6. Triple Retrieval Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "Triple(Fact) 검색 관련 함수\n",
        "- Dense retrieval로 top-k triple 검색\n",
        "- LLM 기반 triple 필터링\n",
        "\"\"\"\n",
        "\n",
        "def retrieve_top_facts(\n",
        "    hippo: HippoRAG, \n",
        "    query_text: str, \n",
        "    top_k: int = 5\n",
        ") -> Tuple[List[Tuple[str, str, str]], List[float]]:\n",
        "    \"\"\"\n",
        "    Query와 가장 유사한 top-k facts(triples) 검색\n",
        "    \n",
        "    Embedding similarity 기반 dense retrieval\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        query_text: 검색 쿼리\n",
        "        top_k: 반환할 triple 수\n",
        "    \n",
        "    Returns:\n",
        "        (triples 리스트, scores 리스트) 튜플\n",
        "    \"\"\"\n",
        "    # Query embedding\n",
        "    query_emb = hippo.embedding_model.batch_encode(\n",
        "        [query_text], \n",
        "        instruction=\"query_to_fact\", \n",
        "        norm=True\n",
        "    )[0]\n",
        "    \n",
        "    # Similarity 계산\n",
        "    sims = hippo.fact_embeddings @ query_emb\n",
        "    idxs = np.argsort(sims)[::-1][:top_k]\n",
        "    \n",
        "    # Fact 정보 추출\n",
        "    fact_ids = [hippo.fact_node_keys[i] for i in idxs]\n",
        "    rows = hippo.fact_embedding_store.get_rows(fact_ids)\n",
        "    facts = [tuple(eval(rows[fid][\"content\"])) for fid in fact_ids]\n",
        "    \n",
        "    return facts, sims[idxs].tolist()\n",
        "\n",
        "\n",
        "def collect_bridge_triples(\n",
        "    hippo: HippoRAG, \n",
        "    bridge_questions: List[str], \n",
        "    top_k_per_bridge: int = None, \n",
        "    max_triples: int = None\n",
        ") -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Step 1.2: Bridge Questions로 초기 Triple 검색\n",
        "    \n",
        "    각 bridge question에 대해 top-k triples를 검색하고,\n",
        "    중복 제거 후 max_triples까지 수집\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        bridge_questions: Atomic bridge questions 리스트\n",
        "        top_k_per_bridge: 각 question당 검색할 triple 수\n",
        "        max_triples: 최대 수집 triple 수\n",
        "    \n",
        "    Returns:\n",
        "        수집된 unique triples 리스트\n",
        "    \"\"\"\n",
        "    top_k_per_bridge = top_k_per_bridge or CONFIG.top_k_per_bridge\n",
        "    max_triples = max_triples or CONFIG.max_bridge_triples\n",
        "    \n",
        "    gathered, seen = [], set()\n",
        "    \n",
        "    for bridge_q in bridge_questions:\n",
        "        bridge_q = bridge_q.strip()\n",
        "        if not bridge_q:\n",
        "            continue\n",
        "        \n",
        "        top_facts, _ = retrieve_top_facts(hippo, bridge_q, top_k=top_k_per_bridge)\n",
        "        normalized = filter_invalid_triples([list(fact) for fact in top_facts])\n",
        "        \n",
        "        for triple in normalized:\n",
        "            triple_tuple = tuple(triple)\n",
        "            if triple_tuple in seen:\n",
        "                continue\n",
        "            seen.add(triple_tuple)\n",
        "            gathered.append(triple_tuple)\n",
        "            \n",
        "            if len(gathered) >= max_triples:\n",
        "                return gathered\n",
        "    \n",
        "    return gathered\n",
        "\n",
        "\n",
        "def llm_filter_triples(\n",
        "    hippo: HippoRAG, \n",
        "    query: str, \n",
        "    candidate_triples: List[Tuple[str, str, str]]\n",
        ") -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    LLM을 사용한 Triple 필터링\n",
        "    \n",
        "    DSPy Filter를 통해 query와 관련 없는 triples 제거\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        query: 원본/sub 질문\n",
        "        candidate_triples: 필터링할 후보 triples\n",
        "    \n",
        "    Returns:\n",
        "        필터링된 triples 리스트\n",
        "    \"\"\"\n",
        "    if not candidate_triples:\n",
        "        return []\n",
        "    \n",
        "    filter_obj = DSPyFilter(hippo)\n",
        "    \n",
        "    cfg = BaseConfig(\n",
        "        save_dir=os.path.join(hippo.global_config.save_dir, \"bridge_filter\"),\n",
        "        llm_base_url=hippo.global_config.llm_base_url,\n",
        "        llm_name=hippo.global_config.llm_name,\n",
        "        dataset=\"adhoc\",\n",
        "        embedding_model_name=hippo.global_config.embedding_model_name,\n",
        "    )\n",
        "    llm = CacheOpenAI.from_experiment_config(cfg)\n",
        "    \n",
        "    # 필터 요청\n",
        "    payload = {\"fact\": [list(triple) for triple in candidate_triples]}\n",
        "    messages = deepcopy(filter_obj.message_template)\n",
        "    messages.append({\n",
        "        \"role\": \"user\", \n",
        "        \"content\": filter_obj.one_input_template.format(\n",
        "            question=query, \n",
        "            fact_before_filter=json.dumps(payload)\n",
        "        )\n",
        "    })\n",
        "    \n",
        "    raw, *_ = llm.infer(messages=messages, max_completion_tokens=512, temperature=0.0)\n",
        "    text = raw[0] if isinstance(raw, (list, tuple)) else raw or \"\"\n",
        "    \n",
        "    # 파싱 및 매칭\n",
        "    generated = filter_obj.parse_filter(text)\n",
        "    matched, cand_strings = [], [str(t) for t in candidate_triples]\n",
        "    \n",
        "    for fact in generated:\n",
        "        if isinstance(fact, list) and len(fact) == 3:\n",
        "            triple = tuple(str(x) for x in fact)\n",
        "            try:\n",
        "                idx = cand_strings.index(str(triple))\n",
        "                if candidate_triples[idx] not in matched:\n",
        "                    matched.append(candidate_triples[idx])\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    return matched\n",
        "\n",
        "\n",
        "def collect_sub_question_triples(\n",
        "    hippo: HippoRAG, \n",
        "    sub_questions: List[str], \n",
        "    top_k: int = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Step 1.4: Sub-Questions로 Triple 검색 + LLM 필터링\n",
        "    \n",
        "    각 sub-question(hop)에 대해:\n",
        "    1. Dense retrieval로 top-k triples 검색\n",
        "    2. LLM filter로 관련성 낮은 triples 제거\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        sub_questions: 분해된 sub-questions\n",
        "        top_k: 각 question당 검색할 triple 수\n",
        "    \n",
        "    Returns:\n",
        "        각 hop의 정보를 담은 딕셔너리 리스트\n",
        "    \"\"\"\n",
        "    top_k = top_k or CONFIG.top_k_per_hop\n",
        "    hop_infos = []\n",
        "    \n",
        "    for hop_idx, sub_q in enumerate(sub_questions):\n",
        "        sub_q = sub_q.strip()\n",
        "        if not sub_q:\n",
        "            continue\n",
        "        \n",
        "        # Dense retrieval\n",
        "        top_facts, top_scores = retrieve_top_facts(hippo, sub_q, top_k=top_k)\n",
        "        \n",
        "        # LLM filtering\n",
        "        filtered = llm_filter_triples(hippo, sub_q, top_facts)\n",
        "        \n",
        "        # 필터링된 facts의 scores 매핑\n",
        "        lookup = dict(zip(top_facts, top_scores))\n",
        "        \n",
        "        hop_infos.append({\n",
        "            \"hop\": hop_idx,\n",
        "            \"sub_question\": sub_q,\n",
        "            \"top_facts_before_filter\": top_facts,\n",
        "            \"top_scores_before_filter\": top_scores,\n",
        "            \"filtered_facts\": filtered,\n",
        "            \"filtered_scores\": [lookup.get(fact, 0.0) for fact in filtered],\n",
        "        })\n",
        "    \n",
        "    return hop_infos\n",
        "\n",
        "\n",
        "print(\"✓ Triple 검색 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "89faea78",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Seed Vector 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7. Seed Vector & Index Mapping Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "Fact Seed Vector 생성 및 Triple-Index 매핑 함수\n",
        "\"\"\"\n",
        "\n",
        "def _map_triples_to_indices(\n",
        "    hippo: HippoRAG, \n",
        "    triples: List[Tuple[str, str, str]]\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Triple을 HippoRAG의 fact embedding index로 매핑\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        triples: 매핑할 triples\n",
        "    \n",
        "    Returns:\n",
        "        각 triple에 대응하는 fact index 리스트\n",
        "    \"\"\"\n",
        "    if not triples:\n",
        "        return []\n",
        "    \n",
        "    # Fact ID -> Row 매핑 생성\n",
        "    id_to_row = hippo.fact_embedding_store.get_rows(hippo.fact_node_keys)\n",
        "    \n",
        "    # Triple 문자열 -> Index 룩업 테이블\n",
        "    lookup = {}\n",
        "    for idx, fid in enumerate(hippo.fact_node_keys):\n",
        "        triple = tuple(eval(id_to_row[fid][\"content\"]))\n",
        "        lookup[str(tuple(text_processing(list(triple))))] = idx\n",
        "    \n",
        "    # 매핑\n",
        "    indices = []\n",
        "    for triple in triples:\n",
        "        key = str(tuple(text_processing(list(triple))))\n",
        "        if key in lookup:\n",
        "            indices.append(lookup[key])\n",
        "    \n",
        "    return indices\n",
        "\n",
        "\n",
        "def build_fact_seed_vector(\n",
        "    hop_infos: List[Dict[str, Any]], \n",
        "    hippo: HippoRAG, \n",
        "    aggregation_method: str = None\n",
        ") -> Tuple[np.ndarray, List[int], List[Tuple[str, str, str]]]:\n",
        "    \"\"\"\n",
        "    Step 1.5: Fact Seed Vector 생성\n",
        "    \n",
        "    각 hop의 filtered_facts와 scores를 집계하여\n",
        "    PPR의 seed로 사용할 fact score vector 생성\n",
        "    \n",
        "    Args:\n",
        "        hop_infos: collect_sub_question_triples()의 결과\n",
        "        hippo: HippoRAG 객체\n",
        "        aggregation_method: 점수 집계 방식\n",
        "            - \"sum\": 점수 합산\n",
        "            - \"weighted_sum\": hop 빈도 가중치 적용\n",
        "            - \"max\": 최대값 사용\n",
        "    \n",
        "    Returns:\n",
        "        (fact_scores_vec, seed_indices, seed_facts) 튜플\n",
        "    \"\"\"\n",
        "    aggregation_method = aggregation_method or CONFIG.aggregation_method\n",
        "    \n",
        "    fact_scores = np.zeros(len(hippo.fact_embeddings))\n",
        "    fact_seed_map, fact_hop_count = {}, {}\n",
        "    \n",
        "    for hop in hop_infos:\n",
        "        for fact, score in zip(hop.get(\"filtered_facts\") or [], hop.get(\"filtered_scores\") or []):\n",
        "            idxs = _map_triples_to_indices(hippo, [fact])\n",
        "            if not idxs:\n",
        "                continue\n",
        "            \n",
        "            idx = idxs[0]\n",
        "            \n",
        "            if idx not in fact_seed_map:\n",
        "                fact_seed_map[idx] = fact\n",
        "                fact_hop_count[idx] = 0\n",
        "            fact_hop_count[idx] += 1\n",
        "            \n",
        "            # 집계 방식에 따른 점수 계산\n",
        "            if aggregation_method == \"sum\":\n",
        "                fact_scores[idx] += float(score)\n",
        "            elif aggregation_method == \"weighted_sum\":\n",
        "                fact_scores[idx] += float(score) * (1.0 + fact_hop_count[idx] * 0.1)\n",
        "            else:  # max\n",
        "                fact_scores[idx] = max(fact_scores[idx], float(score))\n",
        "    \n",
        "    # 정규화\n",
        "    if fact_scores.sum() > 0:\n",
        "        fact_scores = min_max_normalize(fact_scores)\n",
        "    \n",
        "    seed_indices = list(fact_seed_map.keys())\n",
        "    seed_facts = [fact_seed_map[idx] for idx in seed_indices]\n",
        "    \n",
        "    return fact_scores, seed_indices, seed_facts\n",
        "\n",
        "\n",
        "def build_seed_vector_from_stored(\n",
        "    hippo: HippoRAG, \n",
        "    stored_triples: List[Tuple[str, str, str]], \n",
        "    stored_scores: List[float]\n",
        ") -> Tuple[np.ndarray, List[int], List[Tuple[str, str, str]]]:\n",
        "    \"\"\"\n",
        "    저장된 Triple과 Score로부터 Seed Vector 재생성\n",
        "    \n",
        "    Step 2, 3, 4에서 Memory Bank의 데이터를 사용하여 seed vector 생성\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        stored_triples: 저장된 triples\n",
        "        stored_scores: 각 triple의 점수\n",
        "    \n",
        "    Returns:\n",
        "        (fact_scores_vec, seed_indices, seed_facts) 튜플\n",
        "    \"\"\"\n",
        "    seed_indices = _map_triples_to_indices(hippo, stored_triples)\n",
        "    seed_facts = stored_triples[:len(seed_indices)]\n",
        "    scores_for_indices = stored_scores[:len(seed_indices)]\n",
        "    \n",
        "    fact_scores_vec = np.zeros(len(hippo.fact_embeddings))\n",
        "    for fact_idx, score in zip(seed_indices, scores_for_indices):\n",
        "        fact_scores_vec[fact_idx] = float(score)\n",
        "    \n",
        "    if fact_scores_vec.sum() > 0:\n",
        "        fact_scores_vec = min_max_normalize(fact_scores_vec)\n",
        "    \n",
        "    return fact_scores_vec, seed_indices, seed_facts\n",
        "\n",
        "\n",
        "print(\"✓ Seed Vector 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ffc9c688",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ STPPR & QCEdge 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 8. STPPR & QCEdge Extraction Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "STPPR (Source-Target PPR) 및 QCEdge 추출 함수\n",
        "- RBS (Reverse Backward Search) 알고리즘\n",
        "- Query-Conditioned Edge 계산\n",
        "\"\"\"\n",
        "\n",
        "def run_rbs_backward_search(\n",
        "    graph,\n",
        "    target_node: int,\n",
        "    alpha: float = None,\n",
        "    eps: float = None,\n",
        "    error_type: int = 1\n",
        ") -> Tuple[Dict[int, float], Dict[Tuple[int, int], float]]:\n",
        "    \"\"\"\n",
        "    RBS (Reverse Backward Search) 알고리즘\n",
        "    \n",
        "    Target 노드로부터 역방향으로 PPR 스코어를 전파하여\n",
        "    각 노드의 기여도와 edge flow를 계산\n",
        "    \n",
        "    이를 통해 특정 passage(target)에 대한 각 edge의 중요도 파악 가능\n",
        "    \n",
        "    Args:\n",
        "        graph: igraph Graph 객체\n",
        "        target_node: 역방향 탐색 시작 노드 (passage node)\n",
        "        alpha: Reset probability (높을수록 target 근처에 집중)\n",
        "        eps: 수렴 threshold\n",
        "        error_type: 에러 계산 방식 (1 또는 2)\n",
        "    \n",
        "    Returns:\n",
        "        (final_reserve, edge_flow) 튜플\n",
        "        - final_reserve: 각 노드의 최종 reserve 값\n",
        "        - edge_flow: 각 edge (u, v)의 flow 값\n",
        "    \"\"\"\n",
        "    alpha = alpha or CONFIG.rbs_alpha\n",
        "    eps = eps or CONFIG.rbs_eps\n",
        "    \n",
        "    n = graph.vcount()\n",
        "    residue = [defaultdict(float), defaultdict(float)]\n",
        "    final_reserve = defaultdict(float)\n",
        "    edge_flow = defaultdict(float)\n",
        "    visited = set()\n",
        "    candidate_set = [[], []]\n",
        "    candidate_count = [0, 0]\n",
        "    \n",
        "    # 초기화\n",
        "    level = 0\n",
        "    residue[0][target_node] = 1.0\n",
        "    candidate_set[0] = [target_node]\n",
        "    candidate_count[0] = 1\n",
        "    \n",
        "    # 최대 레벨 계산\n",
        "    L = int(math.ceil(math.log(eps) / math.log(1 - alpha))) + 1 if alpha < 1.0 else 1\n",
        "    \n",
        "    while level <= L:\n",
        "        current_level = level % 2\n",
        "        next_level = (level + 1) % 2\n",
        "        \n",
        "        candidate_cnt = candidate_count[current_level]\n",
        "        if candidate_cnt == 0:\n",
        "            break\n",
        "        \n",
        "        candidate_count[current_level] = 0\n",
        "        \n",
        "        for j in range(candidate_cnt):\n",
        "            temp_node = candidate_set[current_level][j]\n",
        "            temp_r = residue[current_level][temp_node]\n",
        "            residue[current_level][temp_node] = 0.0\n",
        "            \n",
        "            if temp_node not in visited:\n",
        "                visited.add(temp_node)\n",
        "            \n",
        "            # Reserve 업데이트\n",
        "            final_reserve[temp_node] += alpha * temp_r\n",
        "            \n",
        "            if level == L:\n",
        "                continue\n",
        "            \n",
        "            # 이웃 노드로 전파\n",
        "            neighbors = graph.neighbors(temp_node, mode='all')\n",
        "            \n",
        "            for neighbor in neighbors:\n",
        "                degree = graph.degree(neighbor, mode='all')\n",
        "                if degree == 0:\n",
        "                    continue\n",
        "                \n",
        "                incre = temp_r * (1 - alpha) / float(degree)\n",
        "                ran = random.random()\n",
        "                \n",
        "                # Edge flow 및 residue 업데이트\n",
        "                if error_type == 1:\n",
        "                    if math.sqrt(degree) * eps <= (1 - alpha) * temp_r:\n",
        "                        edge_flow[(neighbor, temp_node)] += incre\n",
        "                        residue[next_level][neighbor] += incre\n",
        "                    else:\n",
        "                        if math.sqrt(degree) * ran * eps <= (1 - alpha) * temp_r:\n",
        "                            ran_incre = eps / math.sqrt(degree)\n",
        "                            edge_flow[(neighbor, temp_node)] += ran_incre\n",
        "                            residue[next_level][neighbor] += ran_incre\n",
        "                        else:\n",
        "                            break\n",
        "                else:\n",
        "                    if degree * eps <= (1 - alpha) * temp_r:\n",
        "                        edge_flow[(neighbor, temp_node)] += incre\n",
        "                        residue[next_level][neighbor] += incre\n",
        "                    else:\n",
        "                        if degree * eps * ran <= (1 - alpha) * temp_r:\n",
        "                            ran_incre = eps\n",
        "                            edge_flow[(neighbor, temp_node)] += ran_incre\n",
        "                            residue[next_level][neighbor] += ran_incre\n",
        "                        else:\n",
        "                            break\n",
        "                \n",
        "                # 다음 레벨 후보 추가\n",
        "                if residue[next_level][neighbor] > eps and neighbor not in candidate_set[next_level]:\n",
        "                    candidate_set[next_level].append(neighbor)\n",
        "                    candidate_count[next_level] += 1\n",
        "        \n",
        "        level += 1\n",
        "    \n",
        "    return dict(final_reserve), dict(edge_flow)\n",
        "\n",
        "\n",
        "def extract_qc_edges(\n",
        "    hippo: HippoRAG,\n",
        "    question: str,\n",
        "    fact_scores_vec: np.ndarray,\n",
        "    seed_indices: List[int],\n",
        "    seed_facts: List[Tuple[str, str, str]],\n",
        "    top_k_passages: int = None,\n",
        "    top_k_edges: int = None,\n",
        "    eps: float = 1e-6,\n",
        "    step_label: str = \"unknown\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Step 1.7 / 2.4 / 3.x: QCEdge (Query-Conditioned Edge) 추출\n",
        "    \n",
        "    PPR로 top-k passages를 검색한 후,\n",
        "    각 passage에 대해 RBS backward search를 수행하여\n",
        "    query와 연관된 중요 edges(QCEdge)를 추출\n",
        "    \n",
        "    QCEdge 값 = PPR forward score × RBS backward flow\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        question: 원본 질문\n",
        "        fact_scores_vec: Seed fact scores\n",
        "        seed_indices: Seed fact indices\n",
        "        seed_facts: Seed facts\n",
        "        top_k_passages: STPPR 실행할 passage 수\n",
        "        top_k_edges: 저장할 QCEdge 수\n",
        "        eps: importance 계산 시 epsilon\n",
        "        step_label: step 라벨\n",
        "    \n",
        "    Returns:\n",
        "        QCEdge 정보 딕셔너리 리스트\n",
        "    \"\"\"\n",
        "    top_k_passages = top_k_passages or CONFIG.top_k_passages_stppr\n",
        "    top_k_edges = top_k_edges or CONFIG.top_k_edges\n",
        "    \n",
        "    if fact_scores_vec.sum() == 0 or not seed_indices:\n",
        "        return []\n",
        "    \n",
        "    # PPR 실행하여 top-k passages 검색\n",
        "    doc_ids, doc_scores, _, _ = hippo.graph_search_with_fact_entities(\n",
        "        query=question,\n",
        "        link_top_k=hippo.global_config.linking_top_k,\n",
        "        query_fact_scores=fact_scores_vec,\n",
        "        top_k_facts=seed_facts,\n",
        "        top_k_fact_indices=seed_indices,\n",
        "        passage_node_weight=hippo.global_config.passage_node_weight,\n",
        "    )\n",
        "    \n",
        "    query_conditioned_edges = defaultdict(float)\n",
        "    \n",
        "    # 각 top-k passage에 대해 RBS 실행 (STPPR)\n",
        "    for passage_idx in doc_ids[:top_k_passages]:\n",
        "        passage_node_id = hippo.passage_node_idxs[passage_idx]\n",
        "        \n",
        "        _, edge_flow = run_rbs_backward_search(\n",
        "            graph=hippo.graph,\n",
        "            target_node=passage_node_id,\n",
        "        )\n",
        "        \n",
        "        # Forward score와 backward flow를 결합\n",
        "        forward_score = doc_scores[list(doc_ids).index(passage_idx)]\n",
        "        for (u, v), flow in edge_flow.items():\n",
        "            qc_edge_value = forward_score * flow\n",
        "            query_conditioned_edges[(u, v)] += qc_edge_value\n",
        "    \n",
        "    if len(query_conditioned_edges) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Top-k edges 선택\n",
        "    sorted_qc_edges = sorted(query_conditioned_edges.items(), key=lambda x: x[1], reverse=True)\n",
        "    max_qc_edge = max(query_conditioned_edges.values())\n",
        "    top_k_sorted = sorted_qc_edges[:top_k_edges]\n",
        "    \n",
        "    # 결과 구성\n",
        "    memory_bank_entries = []\n",
        "    for (u, v), qc_edge_value in top_k_sorted:\n",
        "        importance = (qc_edge_value / max_qc_edge) + eps\n",
        "        memory_bank_entries.append({\n",
        "            'step': step_label,\n",
        "            'edge': [int(u), int(v)],\n",
        "            'importance': float(importance),\n",
        "            'qc_edge_value': float(qc_edge_value)\n",
        "        })\n",
        "    \n",
        "    return memory_bank_entries\n",
        "\n",
        "\n",
        "print(\"✓ STPPR & QCEdge 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b357f5aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Graph Strengthening 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 9. Graph Strengthening Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "QCEdge를 활용한 그래프 강화 함수\n",
        "- Step 2: 단순 QCEdge 강화\n",
        "- Step 3, 4: Intersection + Extra QCEdge 강화\n",
        "\"\"\"\n",
        "\n",
        "def strengthen_edges_with_qc_edges(\n",
        "    graph,\n",
        "    qc_edges: List[Dict[str, Any]],\n",
        "    theta: float = None,\n",
        "    weight_upper_bound: float = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Step 2: QCEdge로 그래프 엣지 가중치 강화\n",
        "    \n",
        "    수식: new_weight = current_weight × (1.0 + √(importance × theta))\n",
        "    \n",
        "    Args:\n",
        "        graph: igraph Graph 객체 (원본)\n",
        "        qc_edges: QCEdge 정보 리스트\n",
        "        theta: 강화 강도 (높을수록 더 강한 강화)\n",
        "        weight_upper_bound: 가중치 상한\n",
        "    \n",
        "    Returns:\n",
        "        강화된 그래프 복사본\n",
        "    \"\"\"\n",
        "    theta = theta or CONFIG.theta_step2\n",
        "    weight_upper_bound = weight_upper_bound or CONFIG.wub_step2\n",
        "    \n",
        "    strengthened_graph = graph.copy()\n",
        "    \n",
        "    # Edge -> Importance 매핑\n",
        "    edges_to_strengthen = {\n",
        "        tuple(entry['edge']): entry['importance'] \n",
        "        for entry in qc_edges\n",
        "    }\n",
        "    \n",
        "    for edge in strengthened_graph.es:\n",
        "        u, v = edge.source, edge.target\n",
        "        edge_key = (u, v)\n",
        "        reverse_key = (v, u)\n",
        "        \n",
        "        # 강화 대상 엣지인지 확인\n",
        "        if edge_key in edges_to_strengthen:\n",
        "            importance = edges_to_strengthen[edge_key]\n",
        "        elif reverse_key in edges_to_strengthen:\n",
        "            importance = edges_to_strengthen[reverse_key]\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # 가중치 강화\n",
        "        current_weight = edge['weight'] if 'weight' in edge.attributes() else 1.0\n",
        "        new_weight = current_weight * (1.0 + np.sqrt(importance * theta))\n",
        "        new_weight = min(new_weight, weight_upper_bound)\n",
        "        edge['weight'] = new_weight\n",
        "    \n",
        "    return strengthened_graph\n",
        "\n",
        "\n",
        "def compute_qc_edge_intersection(\n",
        "    qc_edges_step1: List[Dict[str, Any]],\n",
        "    qc_edges_step2: List[Dict[str, Any]]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Step 2.5: 두 단계의 QCEdge 교집합 계산\n",
        "    \n",
        "    두 단계 모두에서 중요했던 edges만 선택\n",
        "    (신뢰도 높은 edges)\n",
        "    \n",
        "    Args:\n",
        "        qc_edges_step1: Step 1에서 추출한 QCEdges\n",
        "        qc_edges_step2: Step 2에서 추출한 QCEdges\n",
        "    \n",
        "    Returns:\n",
        "        교집합 QCEdge 리스트 (importance = 두 단계 중 최소값)\n",
        "    \"\"\"\n",
        "    edges_step1 = {tuple(entry['edge']): entry for entry in qc_edges_step1}\n",
        "    edges_step2 = {tuple(entry['edge']): entry for entry in qc_edges_step2}\n",
        "    \n",
        "    intersection_edges = []\n",
        "    \n",
        "    for edge_key, entry1 in edges_step1.items():\n",
        "        reverse_key = (edge_key[1], edge_key[0])\n",
        "        \n",
        "        if edge_key in edges_step2:\n",
        "            entry2 = edges_step2[edge_key]\n",
        "        elif reverse_key in edges_step2:\n",
        "            entry2 = edges_step2[reverse_key]\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # 교집합 엣지: 두 단계 중 최소 importance 사용\n",
        "        intersection_edges.append({\n",
        "            'edge': list(edge_key),\n",
        "            'importance': min(entry1.get('importance', 0), entry2.get('importance', 0)),\n",
        "            'qc_edge_value': min(entry1.get('qc_edge_value', 0), entry2.get('qc_edge_value', 0)),\n",
        "            'step1_importance': entry1.get('importance', 0),\n",
        "            'step2_importance': entry2.get('importance', 0),\n",
        "        })\n",
        "    \n",
        "    return intersection_edges\n",
        "\n",
        "\n",
        "def select_extra_qc_edges_by_percentile(\n",
        "    qc_edges_step1: List[Dict[str, Any]],\n",
        "    percentile: float = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Step 3.1: Percentile 기준으로 Extra QCEdge 선택\n",
        "    \n",
        "    Step 1의 QCEdge 중 상위 percentile에 해당하는 edges 선택\n",
        "    \n",
        "    Args:\n",
        "        qc_edges_step1: Step 1에서 추출한 QCEdges\n",
        "        percentile: 선택할 상위 percentile (기본: 50)\n",
        "    \n",
        "    Returns:\n",
        "        선택된 Extra QCEdge 리스트\n",
        "    \"\"\"\n",
        "    percentile = percentile or CONFIG.percentile_step3\n",
        "    \n",
        "    if not qc_edges_step1:\n",
        "        return []\n",
        "    \n",
        "    # qc_edge_value 기준 정렬\n",
        "    sorted_edges = sorted(qc_edges_step1, key=lambda x: x.get('qc_edge_value', 0), reverse=True)\n",
        "    \n",
        "    # Percentile threshold 계산\n",
        "    values = [e.get('qc_edge_value', 0) for e in sorted_edges]\n",
        "    if not values:\n",
        "        return []\n",
        "    \n",
        "    threshold = np.percentile(values, percentile)\n",
        "    \n",
        "    # Threshold 이상인 edges 선택\n",
        "    extra_edges = [e for e in sorted_edges if e.get('qc_edge_value', 0) >= threshold]\n",
        "    \n",
        "    return extra_edges\n",
        "\n",
        "\n",
        "def strengthen_edges_with_intersection_and_extra(\n",
        "    graph,\n",
        "    intersection_edges: List[Dict[str, Any]],\n",
        "    extra_edges: List[Dict[str, Any]],\n",
        "    theta: float = None,\n",
        "    theta_multiplier: float = None,\n",
        "    weight_upper_bound: float = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Step 3, 4: Intersection QCEdge + Extra QCEdge로 그래프 강화\n",
        "    \n",
        "    effective_theta = theta × theta_multiplier\n",
        "    수식: new_weight = current_weight × (1.0 + √(importance × effective_theta))\n",
        "    \n",
        "    Args:\n",
        "        graph: igraph Graph 객체 (원본)\n",
        "        intersection_edges: 교집합 QCEdges\n",
        "        extra_edges: Extra QCEdges\n",
        "        theta: 기본 theta\n",
        "        theta_multiplier: theta 배수\n",
        "        weight_upper_bound: 가중치 상한\n",
        "    \n",
        "    Returns:\n",
        "        강화된 그래프 복사본\n",
        "    \"\"\"\n",
        "    theta = theta or CONFIG.theta_step3\n",
        "    theta_multiplier = theta_multiplier or CONFIG.theta_mult_step3\n",
        "    weight_upper_bound = weight_upper_bound or CONFIG.wub_step3\n",
        "    \n",
        "    strengthened_graph = graph.copy()\n",
        "    effective_theta = theta * theta_multiplier\n",
        "    \n",
        "    # Edge -> Entry 매핑\n",
        "    intersection_dict = {tuple(entry['edge']): entry for entry in intersection_edges}\n",
        "    extra_dict = {tuple(entry['edge']): entry for entry in extra_edges}\n",
        "    \n",
        "    for edge in strengthened_graph.es:\n",
        "        u, v = edge.source, edge.target\n",
        "        edge_key = (u, v)\n",
        "        reverse_key = (v, u)\n",
        "        \n",
        "        current_weight = edge['weight'] if 'weight' in edge.attributes() else 1.0\n",
        "        \n",
        "        # 교집합 엣지인 경우\n",
        "        if edge_key in intersection_dict or reverse_key in intersection_dict:\n",
        "            entry = intersection_dict.get(edge_key) or intersection_dict.get(reverse_key)\n",
        "            importance = entry.get('importance', 0)\n",
        "            new_weight = current_weight * (1.0 + np.sqrt(importance * effective_theta))\n",
        "            new_weight = min(new_weight, weight_upper_bound)\n",
        "            edge['weight'] = new_weight\n",
        "        # Extra 엣지인 경우\n",
        "        elif edge_key in extra_dict or reverse_key in extra_dict:\n",
        "            entry = extra_dict.get(edge_key) or extra_dict.get(reverse_key)\n",
        "            importance = entry.get('importance', 0)\n",
        "            new_weight = current_weight * (1.0 + np.sqrt(importance * effective_theta))\n",
        "            new_weight = min(new_weight, weight_upper_bound)\n",
        "            edge['weight'] = new_weight\n",
        "    \n",
        "    return strengthened_graph\n",
        "\n",
        "\n",
        "print(\"✓ Graph Strengthening 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5538abb5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 검색 & 평가 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 10. Graph Search & Evaluation Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "PPR 기반 검색 및 성능 평가 함수\n",
        "\"\"\"\n",
        "\n",
        "def run_graph_search(\n",
        "    hippo: HippoRAG,\n",
        "    question: str,\n",
        "    gold: List[str],\n",
        "    fact_scores_vec: np.ndarray,\n",
        "    seed_indices: List[int],\n",
        "    seed_facts: List[Tuple[str, str, str]],\n",
        "    top_k: int = 5\n",
        ") -> RetrievalResult:\n",
        "    \"\"\"\n",
        "    PPR 기반 Passage 검색 및 평가\n",
        "    \n",
        "    Seed vector가 비어있으면 Dense Retrieval로 fallback\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        question: 검색 질문\n",
        "        gold: 정답 문서 리스트\n",
        "        fact_scores_vec: Seed fact scores\n",
        "        seed_indices: Seed fact indices\n",
        "        seed_facts: Seed facts\n",
        "        top_k: 반환할 passage 수\n",
        "    \n",
        "    Returns:\n",
        "        RetrievalResult 객체\n",
        "    \"\"\"\n",
        "    fallback = False\n",
        "    \n",
        "    if fact_scores_vec.sum() == 0 or not seed_indices:\n",
        "        # Dense Retrieval Fallback\n",
        "        doc_ids, doc_scores = hippo.dense_passage_retrieval(question)\n",
        "        fallback = True\n",
        "    else:\n",
        "        # PPR Graph Search\n",
        "        doc_ids, doc_scores, _, _ = hippo.graph_search_with_fact_entities(\n",
        "            query=question,\n",
        "            link_top_k=hippo.global_config.linking_top_k,\n",
        "            query_fact_scores=fact_scores_vec,\n",
        "            top_k_facts=seed_facts,\n",
        "            top_k_fact_indices=seed_indices,\n",
        "            passage_node_weight=hippo.global_config.passage_node_weight,\n",
        "        )\n",
        "    \n",
        "    # Top-k passages 추출\n",
        "    top_passages = [\n",
        "        hippo.chunk_embedding_store.get_row(hippo.passage_node_keys[idx])[\"content\"] \n",
        "        for idx in doc_ids[:top_k]\n",
        "    ]\n",
        "    top_scores = doc_scores[:top_k].tolist()\n",
        "    \n",
        "    # 평가\n",
        "    retrieved_set = {p.strip() for p in top_passages}\n",
        "    gold_set = {g.strip() for g in gold}\n",
        "    \n",
        "    if gold_set:\n",
        "        overlap = retrieved_set & gold_set\n",
        "        recall = len(overlap) / len(gold_set)\n",
        "        hit = int(bool(overlap))\n",
        "    else:\n",
        "        recall, hit = 0.0, 0\n",
        "    \n",
        "    return RetrievalResult(\n",
        "        passages=top_passages,\n",
        "        scores=top_scores,\n",
        "        recall=recall,\n",
        "        hit=hit,\n",
        "        fallback_used=fallback\n",
        "    )\n",
        "\n",
        "\n",
        "def run_fallback_search(\n",
        "    hippo: HippoRAG,\n",
        "    question: str,\n",
        "    gold: List[str],\n",
        "    top_k: int = 5\n",
        ") -> RetrievalResult:\n",
        "    \"\"\"\n",
        "    Dense Retrieval Fallback 검색\n",
        "    \n",
        "    Memory bank에 데이터가 없는 경우 사용\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        question: 검색 질문\n",
        "        gold: 정답 문서 리스트\n",
        "        top_k: 반환할 passage 수\n",
        "    \n",
        "    Returns:\n",
        "        RetrievalResult 객체\n",
        "    \"\"\"\n",
        "    doc_ids, doc_scores = hippo.dense_passage_retrieval(question)\n",
        "    \n",
        "    top_passages = [\n",
        "        hippo.chunk_embedding_store.get_row(hippo.passage_node_keys[idx])[\"content\"] \n",
        "        for idx in doc_ids[:top_k]\n",
        "    ]\n",
        "    top_scores = doc_scores[:top_k].tolist()\n",
        "    \n",
        "    retrieved_set = {p.strip() for p in top_passages}\n",
        "    gold_set = {g.strip() for g in gold}\n",
        "    \n",
        "    if gold_set:\n",
        "        overlap = retrieved_set & gold_set\n",
        "        recall = len(overlap) / len(gold_set)\n",
        "        hit = int(bool(overlap))\n",
        "    else:\n",
        "        recall, hit = 0.0, 0\n",
        "    \n",
        "    return RetrievalResult(\n",
        "        passages=top_passages,\n",
        "        scores=top_scores,\n",
        "        recall=recall,\n",
        "        hit=hit,\n",
        "        fallback_used=True\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"✓ 검색 & 평가 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "894663ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Step 실행 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 11. Step Execution Functions\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "각 Step별 파이프라인 실행 함수\n",
        "- Step 1 (250): 초기 검색 + QCEdge 추출\n",
        "- Step 2 (500): QCEdge 강화 + Intersection 계산\n",
        "- Step 3 (750): Intersection + Extra 강화\n",
        "- Step 4 (1000): 더 강한 강화\n",
        "\"\"\"\n",
        "\n",
        "def run_step1(\n",
        "    hippo: HippoRAG,\n",
        "    queries: List[str],\n",
        "    gold_docs: List[List[str]],\n",
        "    config: PipelineConfig = None,\n",
        "    verbose: bool = True\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any], List[Dict[str, Any]], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Step 1 (Step 250): 초기 검색 및 QCEdge 추출\n",
        "    \n",
        "    파이프라인:\n",
        "    1. Atomic Bridge Question 추출\n",
        "    2. Bridge Triple 검색\n",
        "    3. Context-aware Query Decomposition\n",
        "    4. Sub-Question Triple 검색 + LLM 필터링\n",
        "    5. Fact Seed Vector 생성\n",
        "    6. PPR → Passage 검색\n",
        "    7. STPPR → QCEdge 추출\n",
        "    \n",
        "    Returns:\n",
        "        (memory_bank, buffer, log_records, metrics) 튜플\n",
        "    \"\"\"\n",
        "    config = config or CONFIG\n",
        "    memory_bank = {}\n",
        "    buffer = {}\n",
        "    log_records = []\n",
        "    recalls, hits = [], []\n",
        "    \n",
        "    for idx, question in enumerate(queries):\n",
        "        if verbose and ((idx + 1) % 10 == 0 or (idx + 1) == len(queries)):\n",
        "            print(f\"  [Step 250] {idx + 1}/{len(queries)} ({100 * (idx + 1) / len(queries):.1f}%)\")\n",
        "        \n",
        "        # 1. Atomic Bridge Question 추출\n",
        "        atomic_items = extract_atomic_bridge_questions(question, hippo.global_config)\n",
        "        bridge_questions = [item.question for item in atomic_items] or [question]\n",
        "        \n",
        "        # 2. Bridge Triple 검색\n",
        "        bridge_triples = collect_bridge_triples(hippo, bridge_questions)\n",
        "        \n",
        "        # 3. Query Decomposition\n",
        "        context_bundle = build_context_bundle(question, atomic_items, bridge_triples)\n",
        "        sub_questions = decompose_query(question, context_bundle, hippo.global_config) or bridge_questions\n",
        "        \n",
        "        # 4. Sub-Question Triple 검색 + 필터링\n",
        "        hop_infos = collect_sub_question_triples(hippo, sub_questions)\n",
        "        \n",
        "        # 5. Seed Vector 생성\n",
        "        fact_scores_vec, seed_indices, seed_facts = build_fact_seed_vector(hop_infos, hippo)\n",
        "        \n",
        "        # Triple + Score 저장용\n",
        "        triple_to_score = {}\n",
        "        for hop_info in hop_infos:\n",
        "            for fact, score in zip(hop_info.get(\"filtered_facts\", []), hop_info.get(\"filtered_scores\", [])):\n",
        "                fact_tuple = tuple(fact)\n",
        "                if fact_tuple not in triple_to_score:\n",
        "                    triple_to_score[fact_tuple] = float(score)\n",
        "                else:\n",
        "                    triple_to_score[fact_tuple] = max(triple_to_score[fact_tuple], float(score))\n",
        "        \n",
        "        stored_triples_with_scores = [\n",
        "            {\"triple\": list(triple), \"score\": score} \n",
        "            for triple, score in triple_to_score.items()\n",
        "        ]\n",
        "        \n",
        "        # 6. PPR → Passage\n",
        "        result = run_graph_search(\n",
        "            hippo=hippo,\n",
        "            question=question,\n",
        "            gold=gold_docs[idx],\n",
        "            fact_scores_vec=fact_scores_vec,\n",
        "            seed_indices=seed_indices,\n",
        "            seed_facts=seed_facts,\n",
        "        )\n",
        "        \n",
        "        # 7. STPPR → QCEdge 추출\n",
        "        qc_edges = extract_qc_edges(\n",
        "            hippo=hippo,\n",
        "            question=question,\n",
        "            fact_scores_vec=fact_scores_vec,\n",
        "            seed_indices=seed_indices,\n",
        "            seed_facts=seed_facts,\n",
        "            step_label=\"250\"\n",
        "        )\n",
        "        \n",
        "        # Memory Bank 저장\n",
        "        memory_bank[question] = {\n",
        "            \"retrieved_triples_with_scores\": stored_triples_with_scores,\n",
        "        }\n",
        "        \n",
        "        # Buffer 저장\n",
        "        buffer[question] = {\n",
        "            \"qc_edges_step1\": qc_edges,\n",
        "        }\n",
        "        \n",
        "        # 로그 기록\n",
        "        log_records.append({\n",
        "            \"idx\": idx,\n",
        "            \"question\": question,\n",
        "            \"atomic_bridge_questions\": [asdict(item) for item in atomic_items],\n",
        "            \"bridge_questions\": bridge_questions,\n",
        "            \"bridge_triples\": [list(t) for t in bridge_triples],\n",
        "            \"sub_questions\": sub_questions,\n",
        "            \"seed_facts\": [list(t) for t in seed_facts],\n",
        "            \"qc_edges\": qc_edges,\n",
        "            \"retrieved_passages_top5\": result.passages,\n",
        "            \"retrieved_scores_top5\": result.scores,\n",
        "            \"gold_docs\": gold_docs[idx],\n",
        "            \"recall@5\": result.recall,\n",
        "            \"hit@5\": result.hit,\n",
        "            \"fallback_used\": result.fallback_used,\n",
        "        })\n",
        "        \n",
        "        recalls.append(result.recall)\n",
        "        hits.append(result.hit)\n",
        "    \n",
        "    metrics = {\n",
        "        \"mean_recall@5\": float(np.mean(recalls)) if recalls else 0.0,\n",
        "        \"hit_rate@5\": float(np.mean(hits)) if hits else 0.0,\n",
        "    }\n",
        "    \n",
        "    return memory_bank, buffer, log_records, metrics\n",
        "\n",
        "\n",
        "def run_step2(\n",
        "    hippo: HippoRAG,\n",
        "    queries: List[str],\n",
        "    gold_docs: List[List[str]],\n",
        "    memory_bank: Dict[str, Any],\n",
        "    buffer: Dict[str, Any],\n",
        "    config: PipelineConfig = None,\n",
        "    verbose: bool = True\n",
        ") -> Tuple[Dict[str, Any], List[Dict[str, Any]], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Step 2 (Step 500): QCEdge로 그래프 강화 + Intersection 계산\n",
        "    \n",
        "    파이프라인:\n",
        "    1. Step 1의 QCEdge로 그래프 강화\n",
        "    2. 저장된 Triple로 Seed Vector 재생성\n",
        "    3. PPR → Passage 검색\n",
        "    4. STPPR → QCEdge 추출 (Step 2)\n",
        "    5. Intersection QCEdge 계산\n",
        "    \n",
        "    Returns:\n",
        "        (updated_buffer, log_records, metrics) 튜플\n",
        "    \"\"\"\n",
        "    config = config or CONFIG\n",
        "    original_graph = hippo.graph.copy()\n",
        "    log_records = []\n",
        "    recalls, hits = [], []\n",
        "    \n",
        "    for idx, question in enumerate(queries):\n",
        "        if verbose and ((idx + 1) % 10 == 0 or (idx + 1) == len(queries)):\n",
        "            print(f\"  [Step 500] {idx + 1}/{len(queries)} ({100 * (idx + 1) / len(queries):.1f}%)\")\n",
        "        \n",
        "        if question not in memory_bank or question not in buffer:\n",
        "            result = run_fallback_search(hippo, question, gold_docs[idx])\n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": True,\n",
        "            })\n",
        "            recalls.append(result.recall)\n",
        "            hits.append(result.hit)\n",
        "            continue\n",
        "        \n",
        "        mem_data = memory_bank[question]\n",
        "        buffer_data = buffer[question]\n",
        "        qc_edges_step1 = buffer_data.get(\"qc_edges_step1\", [])\n",
        "        \n",
        "        # 1. QCEdge로 그래프 강화\n",
        "        if qc_edges_step1:\n",
        "            hippo.graph = strengthen_edges_with_qc_edges(\n",
        "                original_graph,\n",
        "                qc_edges_step1,\n",
        "                theta=config.theta_step2,\n",
        "                weight_upper_bound=config.wub_step2\n",
        "            )\n",
        "        \n",
        "        # 2. 저장된 Triple로 Seed Vector 재생성\n",
        "        stored_data = mem_data.get(\"retrieved_triples_with_scores\", [])\n",
        "        \n",
        "        if stored_data:\n",
        "            stored_triples = [tuple(item[\"triple\"]) for item in stored_data]\n",
        "            stored_scores = [item[\"score\"] for item in stored_data]\n",
        "            \n",
        "            fact_scores_vec, seed_indices, seed_facts = build_seed_vector_from_stored(\n",
        "                hippo, stored_triples, stored_scores\n",
        "            )\n",
        "            \n",
        "            # 3. PPR → Passage\n",
        "            result = run_graph_search(\n",
        "                hippo=hippo,\n",
        "                question=question,\n",
        "                gold=gold_docs[idx],\n",
        "                fact_scores_vec=fact_scores_vec,\n",
        "                seed_indices=seed_indices,\n",
        "                seed_facts=seed_facts,\n",
        "            )\n",
        "            \n",
        "            # 4. STPPR → QCEdge 추출 (Step 2)\n",
        "            qc_edges_step2 = extract_qc_edges(\n",
        "                hippo=hippo,\n",
        "                question=question,\n",
        "                fact_scores_vec=fact_scores_vec,\n",
        "                seed_indices=seed_indices,\n",
        "                seed_facts=seed_facts,\n",
        "                step_label=\"500\"\n",
        "            )\n",
        "            \n",
        "            # 5. Intersection 계산\n",
        "            intersection_qc_edges = compute_qc_edge_intersection(qc_edges_step1, qc_edges_step2)\n",
        "            \n",
        "            # Buffer 업데이트\n",
        "            buffer[question][\"qc_edges_step2\"] = qc_edges_step2\n",
        "            buffer[question][\"intersection_qc_edges\"] = intersection_qc_edges\n",
        "            \n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"qc_edges_step1\": qc_edges_step1,\n",
        "                \"qc_edges_step2\": qc_edges_step2,\n",
        "                \"intersection_qc_edges\": intersection_qc_edges,\n",
        "                \"retrieved_passages_top5\": result.passages,\n",
        "                \"retrieved_scores_top5\": result.scores,\n",
        "                \"gold_docs\": gold_docs[idx],\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": result.fallback_used,\n",
        "            })\n",
        "        else:\n",
        "            result = run_fallback_search(hippo, question, gold_docs[idx])\n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": True,\n",
        "            })\n",
        "        \n",
        "        recalls.append(result.recall)\n",
        "        hits.append(result.hit)\n",
        "        \n",
        "        # 그래프 복원\n",
        "        hippo.graph = original_graph\n",
        "    \n",
        "    metrics = {\n",
        "        \"mean_recall@5\": float(np.mean(recalls)) if recalls else 0.0,\n",
        "        \"hit_rate@5\": float(np.mean(hits)) if hits else 0.0,\n",
        "    }\n",
        "    \n",
        "    return buffer, log_records, metrics\n",
        "\n",
        "\n",
        "def run_step3_or_4(\n",
        "    hippo: HippoRAG,\n",
        "    queries: List[str],\n",
        "    gold_docs: List[List[str]],\n",
        "    memory_bank: Dict[str, Any],\n",
        "    buffer: Dict[str, Any],\n",
        "    step_num: int,\n",
        "    theta_multiplier: float,\n",
        "    weight_upper_bound: float,\n",
        "    percentile: float = None,\n",
        "    config: PipelineConfig = None,\n",
        "    verbose: bool = True\n",
        ") -> Tuple[List[Dict[str, Any]], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Step 3 (750) / Step 4 (1000): Intersection + Extra QCEdge로 그래프 강화\n",
        "    \n",
        "    파이프라인:\n",
        "    1. Extra QCEdge 선택 (percentile 기준)\n",
        "    2. Intersection + Extra로 그래프 강화\n",
        "    3. PPR → Passage 검색\n",
        "    \n",
        "    Args:\n",
        "        hippo: HippoRAG 객체\n",
        "        queries: 질문 리스트\n",
        "        gold_docs: 정답 문서 리스트\n",
        "        memory_bank: Step 1에서 생성된 memory bank\n",
        "        buffer: QCEdge 정보 버퍼\n",
        "        step_num: step 번호 (750 또는 1000)\n",
        "        theta_multiplier: theta 배수\n",
        "        weight_upper_bound: 가중치 상한\n",
        "        percentile: Extra QCEdge 선택 percentile\n",
        "        config: 파이프라인 설정\n",
        "        verbose: 진행 상황 출력 여부\n",
        "    \n",
        "    Returns:\n",
        "        (log_records, metrics) 튜플\n",
        "    \"\"\"\n",
        "    config = config or CONFIG\n",
        "    percentile = percentile or config.percentile_step3\n",
        "    \n",
        "    original_graph = hippo.graph.copy()\n",
        "    log_records = []\n",
        "    recalls, hits = [], []\n",
        "    \n",
        "    for idx, question in enumerate(queries):\n",
        "        if verbose and ((idx + 1) % 10 == 0 or (idx + 1) == len(queries)):\n",
        "            print(f\"  [Step {step_num}] {idx + 1}/{len(queries)} ({100 * (idx + 1) / len(queries):.1f}%)\")\n",
        "        \n",
        "        if question not in memory_bank or question not in buffer:\n",
        "            result = run_fallback_search(hippo, question, gold_docs[idx])\n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": True,\n",
        "            })\n",
        "            recalls.append(result.recall)\n",
        "            hits.append(result.hit)\n",
        "            continue\n",
        "        \n",
        "        mem_data = memory_bank[question]\n",
        "        buffer_data = buffer[question]\n",
        "        qc_edges_step1 = buffer_data.get(\"qc_edges_step1\", [])\n",
        "        intersection_qc_edges = buffer_data.get(\"intersection_qc_edges\", [])\n",
        "        \n",
        "        stored_data = mem_data.get(\"retrieved_triples_with_scores\", [])\n",
        "        \n",
        "        if stored_data:\n",
        "            stored_triples = [tuple(item[\"triple\"]) for item in stored_data]\n",
        "            stored_scores = [item[\"score\"] for item in stored_data]\n",
        "            \n",
        "            fact_scores_vec, seed_indices, seed_facts = build_seed_vector_from_stored(\n",
        "                hippo, stored_triples, stored_scores\n",
        "            )\n",
        "            \n",
        "            # 1. Extra QCEdge 선택\n",
        "            extra_qc_edges = select_extra_qc_edges_by_percentile(qc_edges_step1, percentile=percentile)\n",
        "            \n",
        "            # 2. Intersection + Extra로 그래프 강화\n",
        "            hippo.graph = strengthen_edges_with_intersection_and_extra(\n",
        "                original_graph,\n",
        "                intersection_qc_edges,\n",
        "                extra_qc_edges,\n",
        "                theta=config.theta_step3,\n",
        "                theta_multiplier=theta_multiplier,\n",
        "                weight_upper_bound=weight_upper_bound\n",
        "            )\n",
        "            \n",
        "            # 3. PPR → Passage\n",
        "            result = run_graph_search(\n",
        "                hippo=hippo,\n",
        "                question=question,\n",
        "                gold=gold_docs[idx],\n",
        "                fact_scores_vec=fact_scores_vec,\n",
        "                seed_indices=seed_indices,\n",
        "                seed_facts=seed_facts,\n",
        "            )\n",
        "            \n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"percentile\": percentile,\n",
        "                \"theta_multiplier\": theta_multiplier,\n",
        "                \"weight_upper_bound\": weight_upper_bound,\n",
        "                \"intersection_qc_edges\": intersection_qc_edges,\n",
        "                \"extra_qc_edges\": extra_qc_edges,\n",
        "                \"retrieved_passages_top5\": result.passages,\n",
        "                \"retrieved_scores_top5\": result.scores,\n",
        "                \"gold_docs\": gold_docs[idx],\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": result.fallback_used,\n",
        "            })\n",
        "            \n",
        "            # 그래프 복원\n",
        "            hippo.graph = original_graph.copy()\n",
        "        else:\n",
        "            result = run_fallback_search(hippo, question, gold_docs[idx])\n",
        "            log_records.append({\n",
        "                \"idx\": idx,\n",
        "                \"question\": question,\n",
        "                \"recall@5\": result.recall,\n",
        "                \"hit@5\": result.hit,\n",
        "                \"fallback_used\": True,\n",
        "            })\n",
        "        \n",
        "        recalls.append(result.recall)\n",
        "        hits.append(result.hit)\n",
        "    \n",
        "    metrics = {\n",
        "        \"mean_recall@5\": float(np.mean(recalls)) if recalls else 0.0,\n",
        "        \"hit_rate@5\": float(np.mean(hits)) if hits else 0.0,\n",
        "    }\n",
        "    \n",
        "    return log_records, metrics\n",
        "\n",
        "\n",
        "print(\"✓ Step 실행 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f4110ccc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 메인 파이프라인 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 12. Main Pipeline Function\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "전체 파이프라인 실행 함수\n",
        "Step 250 → 500 → 750 → 1000 순차 실행\n",
        "\"\"\"\n",
        "\n",
        "def run_full_pipeline(\n",
        "    dataset_name: str = None,\n",
        "    config: PipelineConfig = None,\n",
        "    query_list: List[str] = None,\n",
        "    step_to_hippo: Dict[int, HippoRAG] = None,\n",
        "    verbose: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    전체 Continual Learning 파이프라인 실행\n",
        "    \n",
        "    Step 250 (초기 검색) → Step 500 (QCEdge 강화) \n",
        "    → Step 750 (Intersection+Extra 강화, θ×15, wub=6) \n",
        "    → Step 1000 (강한 강화, θ×30, wub=20)\n",
        "    \n",
        "    Args:\n",
        "        config: 파이프라인 설정\n",
        "        query_list: 특정 쿼리 리스트 (None이면 전체 데이터셋)\n",
        "        step_to_hippo: 미리 로드한 HippoRAG 객체들 (step -> hippo)\n",
        "        verbose: 진행 상황 출력 여부\n",
        "    \n",
        "    Returns:\n",
        "        Step별 성능 요약 DataFrame\n",
        "    \"\"\"\n",
        "    config = config or CONFIG\n",
        "    output_dir = config.get_output_dir(dataset_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # 데이터 로드\n",
        "    if verbose:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"데이터 로드 중...\")\n",
        "    dataset_name = dataset_name or (CONFIG.dataset_names[0] if CONFIG.dataset_names else \"musique\")\n",
        "    samples = load_samples(dataset_name)\n",
        "    all_queries = get_queries(samples)\n",
        "    all_gold_docs = get_gold_docs(samples, dataset_name)\n",
        "    \n",
        "    # 쿼리 필터링\n",
        "    if query_list:\n",
        "        queries, gold_docs = [], []\n",
        "        lookup = {q: i for i, q in enumerate(all_queries)}\n",
        "        for query in query_list[:config.max_samples]:\n",
        "            if query in lookup:\n",
        "                idx = lookup[query]\n",
        "                queries.append(query)\n",
        "                gold_docs.append(all_gold_docs[idx])\n",
        "    else:\n",
        "        queries = all_queries[:config.max_samples]\n",
        "        gold_docs = all_gold_docs[:config.max_samples]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  총 {len(queries)}개 쿼리 로드 완료\")\n",
        "    \n",
        "    summaries = []\n",
        "    \n",
        "    # ========================\n",
        "    # Step 1: Step 250\n",
        "    # ========================\n",
        "    memory_bank_path = Path(output_dir) / \"memory_bank.json\"\n",
        "    buffer_path = Path(output_dir) / \"buffer.json\"\n",
        "    step250_log_path = Path(output_dir) / \"step_250_log.json\"\n",
        "    \n",
        "    if memory_bank_path.exists() and buffer_path.exists() and step250_log_path.exists():\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 250] 캐시된 결과 로드 중...\")\n",
        "        memory_bank = load_json(str(memory_bank_path))\n",
        "        buffer = load_json(str(buffer_path))\n",
        "        step250_log = load_json(str(step250_log_path))\n",
        "        metrics_250 = {\n",
        "            \"mean_recall@5\": np.mean([r[\"recall@5\"] for r in step250_log]),\n",
        "            \"hit_rate@5\": np.mean([r[\"hit@5\"] for r in step250_log]),\n",
        "        }\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 250] 초기 검색 시작...\")\n",
        "        \n",
        "        hippo_250 = step_to_hippo.get(250) if step_to_hippo else None\n",
        "        if hippo_250 is None:\n",
        "            hippo_250 = build_hipporag(250, dataset_name, config)\n",
        "        \n",
        "        memory_bank, buffer, step250_log, metrics_250 = run_step1(\n",
        "            hippo=hippo_250,\n",
        "            queries=queries,\n",
        "            gold_docs=gold_docs,\n",
        "            config=config,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        save_json(memory_bank, str(memory_bank_path))\n",
        "        save_json(buffer, str(buffer_path))\n",
        "        save_json(step250_log, str(step250_log_path))\n",
        "    \n",
        "    summaries.append({\"step\": 250, **metrics_250})\n",
        "    if verbose:\n",
        "        print(f\"  → Recall@5: {metrics_250['mean_recall@5']:.4f}, Hit@5: {metrics_250['hit_rate@5']:.4f}\")\n",
        "    \n",
        "    # ========================\n",
        "    # Step 2: Step 500\n",
        "    # ========================\n",
        "    step500_log_path = Path(output_dir) / \"step_500_log.json\"\n",
        "    \n",
        "    # Check if intersection_qc_edges exists in buffer\n",
        "    has_intersection = any(\n",
        "        isinstance(v, dict) and v.get(\"intersection_qc_edges\")\n",
        "        for v in buffer.values()\n",
        "    )\n",
        "    \n",
        "    if step500_log_path.exists() and has_intersection:\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 500] 캐시된 결과 로드 중...\")\n",
        "        step500_log = load_json(str(step500_log_path))\n",
        "        metrics_500 = {\n",
        "            \"mean_recall@5\": np.mean([r[\"recall@5\"] for r in step500_log]),\n",
        "            \"hit_rate@5\": np.mean([r[\"hit@5\"] for r in step500_log]),\n",
        "        }\n",
        "        buffer = load_json(str(buffer_path))  # Reload with intersection data\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 500] QCEdge 강화 검색 시작...\")\n",
        "            print(f\"  θ={config.theta_step2}, weight_upper_bound={config.wub_step2}\")\n",
        "        \n",
        "        hippo_500 = step_to_hippo.get(500) if step_to_hippo else None\n",
        "        if hippo_500 is None:\n",
        "            hippo_500 = build_hipporag(500, dataset_name, config)\n",
        "        \n",
        "        buffer, step500_log, metrics_500 = run_step2(\n",
        "            hippo=hippo_500,\n",
        "            queries=queries,\n",
        "            gold_docs=gold_docs,\n",
        "            memory_bank=memory_bank,\n",
        "            buffer=buffer,\n",
        "            config=config,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        save_json(buffer, str(buffer_path))\n",
        "        save_json(step500_log, str(step500_log_path))\n",
        "    \n",
        "    summaries.append({\"step\": 500, **metrics_500})\n",
        "    if verbose:\n",
        "        print(f\"  → Recall@5: {metrics_500['mean_recall@5']:.4f}, Hit@5: {metrics_500['hit_rate@5']:.4f}\")\n",
        "    \n",
        "    # ========================\n",
        "    # Step 3: Step 750\n",
        "    # ========================\n",
        "    step750_log_path = Path(output_dir) / \"step_750_log.json\"\n",
        "    \n",
        "    if step750_log_path.exists():\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 750] 캐시된 결과 로드 중...\")\n",
        "        step750_log = load_json(str(step750_log_path))\n",
        "        metrics_750 = {\n",
        "            \"mean_recall@5\": np.mean([r[\"recall@5\"] for r in step750_log]),\n",
        "            \"hit_rate@5\": np.mean([r[\"hit@5\"] for r in step750_log]),\n",
        "        }\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 750] Intersection + Extra QCEdge 강화 검색 시작...\")\n",
        "            print(f\"  θ_multiplier={config.theta_mult_step3}, weight_upper_bound={config.wub_step3}\")\n",
        "        \n",
        "        hippo_750 = step_to_hippo.get(750) if step_to_hippo else None\n",
        "        if hippo_750 is None:\n",
        "            hippo_750 = build_hipporag(750, dataset_name, config)\n",
        "        \n",
        "        step750_log, metrics_750 = run_step3_or_4(\n",
        "            hippo=hippo_750,\n",
        "            queries=queries,\n",
        "            gold_docs=gold_docs,\n",
        "            memory_bank=memory_bank,\n",
        "            buffer=buffer,\n",
        "            step_num=750,\n",
        "            theta_multiplier=config.theta_mult_step3,\n",
        "            weight_upper_bound=config.wub_step3,\n",
        "            percentile=config.percentile_step3,\n",
        "            config=config,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        save_json(step750_log, str(step750_log_path))\n",
        "    \n",
        "    summaries.append({\"step\": 750, **metrics_750})\n",
        "    if verbose:\n",
        "        print(f\"  → Recall@5: {metrics_750['mean_recall@5']:.4f}, Hit@5: {metrics_750['hit_rate@5']:.4f}\")\n",
        "    \n",
        "    # ========================\n",
        "    # Step 4: Step 1000\n",
        "    # ========================\n",
        "    step1000_log_path = Path(output_dir) / \"step_1000_log.json\"\n",
        "    \n",
        "    if step1000_log_path.exists():\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 1000] 캐시된 결과 로드 중...\")\n",
        "        step1000_log = load_json(str(step1000_log_path))\n",
        "        metrics_1000 = {\n",
        "            \"mean_recall@5\": np.mean([r[\"recall@5\"] for r in step1000_log]),\n",
        "            \"hit_rate@5\": np.mean([r[\"hit@5\"] for r in step1000_log]),\n",
        "        }\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"[Step 1000] 강한 Intersection + Extra QCEdge 강화 검색 시작...\")\n",
        "            print(f\"  θ_multiplier={config.theta_mult_step4}, weight_upper_bound={config.wub_step4}\")\n",
        "        \n",
        "        hippo_1000 = step_to_hippo.get(1000) if step_to_hippo else None\n",
        "        if hippo_1000 is None:\n",
        "            hippo_1000 = build_hipporag(1000, dataset_name, config)\n",
        "        \n",
        "        step1000_log, metrics_1000 = run_step3_or_4(\n",
        "            hippo=hippo_1000,\n",
        "            queries=queries,\n",
        "            gold_docs=gold_docs,\n",
        "            memory_bank=memory_bank,\n",
        "            buffer=buffer,\n",
        "            step_num=1000,\n",
        "            theta_multiplier=config.theta_mult_step4,\n",
        "            weight_upper_bound=config.wub_step4,\n",
        "            percentile=config.percentile_step4,\n",
        "            config=config,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        save_json(step1000_log, str(step1000_log_path))\n",
        "    \n",
        "    summaries.append({\"step\": 1000, **metrics_1000})\n",
        "    if verbose:\n",
        "        print(f\"  → Recall@5: {metrics_1000['mean_recall@5']:.4f}, Hit@5: {metrics_1000['hit_rate@5']:.4f}\")\n",
        "    \n",
        "    # 결과 요약\n",
        "    result_df = pd.DataFrame(summaries).sort_values(\"step\").reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"전체 파이프라인 완료!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(result_df.to_string(index=False))\n",
        "    \n",
        "    return result_df\n",
        "\n",
        "\n",
        "print(\"✓ 메인 파이프라인 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529abebb",
      "metadata": {},
      "source": [
        "# 데이터셋 실험 실행 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6bebed45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "데이터 로드 중...\n",
            "  총 250개 쿼리 로드 완료\n",
            "\n",
            "============================================================\n",
            "[Step 250] 캐시된 결과 로드 중...\n",
            "  → Recall@5: 0.9480, Hit@5: 0.9920\n",
            "\n",
            "============================================================\n",
            "[Step 500] QCEdge 강화 검색 시작...\n",
            "  θ=15.0, weight_upper_bound=3.0\n"
          ]
        },
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # 예제 1: 단일 데이터셋 실험 (musique)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# CONFIG.dataset_names = [\"musique\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# results_df1 = run_full_pipeline(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 예제 2: 단일 데이터셋 실험 (hotpotqa)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m CONFIG\u001b[38;5;241m.\u001b[39mdataset_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpotqa\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhotpotqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # 예제 3: 단일 데이터셋 실험 (2wikimultihopqa)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# CONFIG.dataset_names = [\"2wikimultihopqa\"]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# results_df2 = run_full_pipeline(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     verbose=True\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[28], line 135\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[0;34m(dataset_name, config, query_list, step_to_hippo, verbose)\u001b[0m\n\u001b[1;32m    133\u001b[0m hippo_500 \u001b[38;5;241m=\u001b[39m step_to_hippo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m500\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m step_to_hippo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hippo_500 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     hippo_500 \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_hipporag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m buffer, step500_log, metrics_500 \u001b[38;5;241m=\u001b[39m run_step2(\n\u001b[1;32m    138\u001b[0m     hippo\u001b[38;5;241m=\u001b[39mhippo_500,\n\u001b[1;32m    139\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m save_json(buffer, \u001b[38;5;28mstr\u001b[39m(buffer_path))\n",
            "Cell \u001b[0;32mIn[20], line 83\u001b[0m, in \u001b[0;36mbuild_hipporag\u001b[0;34m(step, dataset_name, config)\u001b[0m\n\u001b[1;32m     72\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhippo_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/step_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m cfg \u001b[38;5;241m=\u001b[39m BaseConfig(\n\u001b[1;32m     75\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[1;32m     76\u001b[0m     llm_base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     force_openie_from_scratch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m hippo \u001b[38;5;241m=\u001b[39m \u001b[43mHippoRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m hippo\u001b[38;5;241m.\u001b[39mprepare_retrieval_objects()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hippo\n",
            "File \u001b[0;32m/NAS/minyeol/HippoRAG/src/hipporag/HippoRAG.py:125\u001b[0m, in \u001b[0;36mHippoRAG.__init__\u001b[0;34m(self, global_config, save_dir, llm_model_name, llm_base_url, embedding_model_name, embedding_base_url, azure_endpoint, azure_embedding_endpoint)\u001b[0m\n\u001b[1;32m    122\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating working directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworking_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworking_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model: BaseLLM \u001b[38;5;241m=\u001b[39m \u001b[43m_get_llm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mopenie_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenie \u001b[38;5;241m=\u001b[39m OpenIE(llm_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model)\n",
            "File \u001b[0;32m/NAS/minyeol/HippoRAG/src/hipporag/llm/__init__.py:25\u001b[0m, in \u001b[0;36m_get_llm_class\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mllm_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformers/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TransformersLLM(config)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCacheOpenAI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_experiment_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/NAS/minyeol/HippoRAG/src/hipporag/llm/openai_gpt.py:120\u001b[0m, in \u001b[0;36mCacheOpenAI.from_experiment_config\u001b[0;34m(cls, global_config)\u001b[0m\n\u001b[1;32m    118\u001b[0m config_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m global_config\u001b[38;5;241m.\u001b[39mmax_retry_attempts\n\u001b[1;32m    119\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(global_config\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/NAS/minyeol/HippoRAG/src/hipporag/llm/openai_gpt.py:148\u001b[0m, in \u001b[0;36mCacheOpenAI.__init__\u001b[0;34m(self, cache_dir, global_config, cache_filename, high_throughput, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mazure_endpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_client \u001b[38;5;241m=\u001b[39m AzureOpenAI(api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mazure_endpoint\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi-version=\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    151\u001b[0m                                      azure_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mazure_endpoint, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries)\n",
            "File \u001b[0;32m~/.conda/envs/hipporagenv/lib/python3.10/site-packages/openai/_client.py:126\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    124\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "# # 예제 1: 단일 데이터셋 실험 (musique)\n",
        "# CONFIG.dataset_names = [\"musique\"]\n",
        "# results_df1 = run_full_pipeline(\n",
        "#     dataset_name=\"musique\",\n",
        "#     config=CONFIG,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# 예제 2: 단일 데이터셋 실험 (hotpotqa)\n",
        "CONFIG.dataset_names = [\"hotpotqa\"]\n",
        "results_df = run_full_pipeline(\n",
        "    dataset_name=\"hotpotqa\",\n",
        "    config=CONFIG,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# # 예제 3: 단일 데이터셋 실험 (2wikimultihopqa)\n",
        "# CONFIG.dataset_names = [\"2wikimultihopqa\"]\n",
        "# results_df2 = run_full_pipeline(\n",
        "#     dataset_name=\"2wikimultihopqa\",\n",
        "#     config=CONFIG,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# # 예제 4: 여러 데이터셋 자동 실험\n",
        "# CONFIG.dataset_names = [\"musique\", \"hotpotqa\", \"2wikimultihopqa\"]\n",
        "# results = run_experiments_for_datasets(\n",
        "#     dataset_names=[\"musique\", \"hotpotqa\", \"2wikimultihopqa\"],\n",
        "#     config=CONFIG,\n",
        "#     verbose=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cbe44e4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df1.to_csv('musique_CL_QD.csv', index=False)\n",
        "results_df2.to_csv('2wikimultihopqa_CL_QD.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hipporagenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
